{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3f64a3e",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/nhsbsa-data-analytics/coffee-and-coding/blob/master/2025-06-25%20%Introduction%20to%20Unit%20Testing%20in%20Python%3F/2025_06_25_unit_testing_in_python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5745e037",
   "metadata": {},
   "source": [
    "# Coffee & Coding: A test per day keeps the doctor away!\n",
    "- **Date**: Wednesday 25th of June 2025\n",
    "- **Presented by**: Alistair Jones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a617562",
   "metadata": {},
   "source": [
    "## Overview\n",
    "This Coffee & Coding introduces unit tests and explores how to design and implement effective tests in analytical work. \n",
    "\n",
    "This will be covered in two parts: \n",
    "\n",
    "1. The first part will outline the key concepts and considerations, agnostic of any specific tools or technologies. This should hopefully provide something for everyone involved in analytical work, regardless of whether you prefer code or low code solutions. \n",
    "2. The second part will focus on the technical implementation of unit tests in Python, with practical examples using the PyTest framework. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac45599f",
   "metadata": {},
   "source": [
    "## Part 1: Key Concepts of Unit Testing\n",
    "\n",
    "Note: references to 'code' in this section are intended to be generic, covering coding languages such as Python or R but also low- and no-code solutions like Excel and Alteryx."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40e9fc7",
   "metadata": {},
   "source": [
    "### What is Unit Testing?\n",
    "\n",
    "Unit testing is a software testing method where individual units or components of a program are tested in isolation to ensure they work as expected.\n",
    "\n",
    "The central question that unit testing address is: **given some inputs, does our code or workflow behave as expected?** This mean returning an expected output, raising an error, writing a database record, printing some output, etc.\n",
    "\n",
    "In analytical work, unit testing makes sure that the different parts of our analysis - such as field definitions, transformations and aggregations - produce the expected results, while handling any unexpected data issues - such as missing or null values, or values outside of expected ranges."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e583e3",
   "metadata": {},
   "source": [
    "### Why does Unit Testing Matter?\n",
    "\n",
    "Unit testing is extremely important in developing, maintaining and quality assuring analytical code:\n",
    "\n",
    "- **Confidence:** Helps ensure your code works correctly so that you can be confident in the results of your analysis.\n",
    "- **Debugging:** When errors occur, tests help you identify them and locate them, which makes debugging simpler and quicker.\n",
    "- **Documentation:** Tests serve as examples of how your code is intended to be used which helps it be understood and reused by someone else (including a future version of yourself!).\n",
    "- **Refactoring:** Makes it safer to improve code (aka refactoring) by making it quick and easy to check if it still works.\n",
    "\n",
    "Be kind to your colleagues (and your future self) and write some tests!  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ab5399",
   "metadata": {},
   "source": [
    "### How do we write Unit Tests?\n",
    "\n",
    "Imagine we have calculation that we want to use in our analysis: for example, the following formula could be used calculate the total cost of a collection of drugs:\n",
    "```\n",
    "total_cost = cost_per_item x number_of_items\n",
    "```\n",
    "\n",
    "Before we use this calculation to produce results, it's a good idea to check that it works! \n",
    "\n",
    "One approach might be to simple manually provide inputs and check we get the right outputs: this is effectively a form of unit testing, just without the reusability element, since we need to manually redo this process each time we want to test.\n",
    "This is where unit testing helps.\n",
    "\n",
    "#### Decoupling 'what' from 'how'\n",
    "At the NHSBSA, the calculation above might have been implemented using:\n",
    "- An Excel Formula\n",
    "- An Alteryx Function\n",
    "- A SQL Macro\n",
    "- A Python or R function\n",
    "\n",
    "Putting *how* we would implement it to one side, let's think about the logic behind *what* we are trying to do to test our code: we want to check that **given certain inputs** (number and cost of items) that our calculation **returns the expected output** (the total cost).\n",
    "\n",
    "This decoupling (*what* from *how*) generally helps us write higher quality tests (and code), by making sure we focus on the most important factor (that the logic is correct) rather than specific implementation details of a particular tool or technology."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21dc4fd",
   "metadata": {},
   "source": [
    "### Unit Test Cases\n",
    "\n",
    "There are usually three main types of scenarios or 'test cases' that we want to check:\n",
    "1. **Expected**: the so-called 'happy path' where inputs are within the normal range of values we expect our calculation to handle and we want to check we get the expected result.\n",
    "2. **Edge**: we should check our calculation at the boundaries of value ranges we can reasonably expect in the data. For example, if our calculation should handle positive integers up to 100 (exclusive), then we want to check the behaviour at `1` and `99`.\n",
    "3. **Error**: finally, we need to ensure our calculation handles 'bad' inputs correctly - this could mean values outside of the expected operating ranges (like a negative number when only positive numbers are expected), missing or invalid values (like nulls or strings in a numeric field) or poorly formatted values (e.g. `2025-05-25` vs `25/05/2025`)\n",
    "    \n",
    "Each type of test case essentially checks for a particular failure mode if our calculation does not work as expected, which helps us understand what might be failing and why so that we can fix it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb4d8ba",
   "metadata": {},
   "source": [
    "#### Example Test Cases\n",
    "\n",
    "The table below shows some plausible example test cases for the total cost calculation, with the expected output, actual output (imagining we had implemented it somehow) and the result (pass or fail):\n",
    "\n",
    "<div style=\"margin-left: auto; margin-right: auto; width: 70%\">\n",
    "\n",
    "| Inputs (Items, Cost per item) | Expected Output | Actual Output | Type         | Test Result |\n",
    "|------------------------------|-----------------|---------------|--------------|-------------|\n",
    "| (3, £8.60)                   | £25.80          | £25.80        | Expected     | Pass        |\n",
    "| (0, £9.35)                   | £0.00           | £0.00         | Edge         | Pass        |\n",
    "| (-2, £7.20)                  | Error           | Error         | Error        | Pass        |\n",
    "| **(10, £8.60)**              | **£86.00**      | **£80.00**    | **Expected** | **Fail**    |\n",
    "| **(null, £5.30)**            | **Error**       | **£0.00**     | **Error**    | **Fail**    |\n",
    "| (1, 'ten pounds 20 pence')   | Error           | Error         | Error        | Pass        |\n",
    "\n",
    "</div>\n",
    "\n",
    "- The failing tests (bold) highlight something that isn't working correctly and help us identify the issues.\n",
    "We can use this for **debugging** the calculations before we use them in anything important! \n",
    "\n",
    "- Once we have fixed the issues, we can have **confidence** in using this calculation to produce outputs.\n",
    "\n",
    "- The test cases show the expected behaviour of the calculation, which serves as a form of **documentation** to help us and others understand what the function is supposed to do.\n",
    "\n",
    "- We can also rerun these tests over and over again to check out calculation still works in future.\n",
    "    - This helps us with **refactoring** to improve quality of existing code, since we can quickly check if anything broke during our changes. \n",
    "    - Or as our calculation evolves we can add new test cases or update existing ones to check that the new logic works as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7351731",
   "metadata": {},
   "source": [
    "### Best Practice Tips\n",
    "\n",
    "- Make tests part of the development process, writing and maintaining them alongside your code or workflows. \n",
    "    - Some advise writing tests *before* you write your code (this is known as 'test-driven development' or 'TDD'), since this makes you focus on *what* your code should do, not *how* it does it (you won't know how it does it because you've not written it yet!) \n",
    "- Keep tests small and focused: remember they should test one aspect of the behaviour, so if you find it difficult to write small tests, it might indicate your code or workflow is doing too much and needs to be broken down into smaller chunks. \n",
    "- Use descriptive names for test functions to ensure tests act as a form of documentation. Tests should read like a story, telling the reader about the expected behaviour. \n",
    "- Consider the different types of test cases. It can be helpful to work through the different cases with a colleague (e.g. a critical friend) before writing any tests to make sure you've covered all the important aspects to test."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f688ab2",
   "metadata": {},
   "source": [
    "### References\n",
    "The following resources explain testing in more detail, provide more examples and onward links to further guidance:\n",
    "- [NHS RAP guidance](https://nhsdigital.github.io/rap-community-of-practice/training_resources/python/unit-testing/)\n",
    "- [Best Practice and Impact guidance](https://best-practice-and-impact.github.io/qa-of-code-guidance/testing_code.html)\n",
    "- (Coming soon!) [NHSBSA Analytical Code Assurance Playbook](https://nhsbsa-data-analytics.github.io/nhsbsa-analytical-code-assurance-playbook/guidance/02-quality-assured/pages/05-tested.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf5b2d9",
   "metadata": {},
   "source": [
    "## Part 2: Writing Unit Tests in Python\n",
    "\n",
    "In this section, we'll write and run unit tests for a simple analytical function written in Python.\n",
    "\n",
    "Although this is implemented in Python, the patterns and terminology should be applicable across a variety of tools and technologies.\n",
    "\n",
    "Note: The steps in this section are written out long-hand to show each tweak to the function we're testing - in reality you would just update the function and rerun the tests!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ffa436d",
   "metadata": {},
   "source": [
    "### Define Our Function\n",
    "We start by defining our function which implements the logic to calculate the total cost of prescription items as outlined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19d27c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_total_cost(number_of_items, cost_per_item):\n",
    "    total_cost = number_of_items * cost_per_item\n",
    "    return total_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b370617f",
   "metadata": {},
   "source": [
    "#### Assertions\n",
    "Assertions are a way to specify the behaviour you expect to observe when some code runs. \n",
    "They are a general concept used across programming languages. \n",
    "The logical structure is to *assert* that a statement is *true*, which will either:\n",
    "1. Do nothing if the statement is really *true*\n",
    "2. Or raise an error if the statement is *false*\n",
    "\n",
    "Assertions are helpful because they alert us to something that isn't working as expected (rather than, say, printing out results which might be easier to skip over).\n",
    "\n",
    "In Python, assertions are written using the `assert` statement: \n",
    "```python\n",
    "assert True, 'You can\\'t see me!'  # This will do nothing\n",
    "assert False, 'Oh no, an error!'  # This will raise an 'AssertionError()'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9019999",
   "metadata": {},
   "source": [
    "Let's look at the first case in the table defined in the previous section:\n",
    "\n",
    "| Inputs (Items, Cost per item) | Expected Output | Actual Output | Type         | Test Result |\n",
    "|------------------------------|-----------------|---------------|--------------|-------------|\n",
    "| (3, £8.60)                   | £25.80          | £25.80        | Expected     | Pass        |\n",
    "\n",
    "We write this test in Python as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5cae4e40",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Expected 25.8, but found 25.799999999999997",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Test that the expected equals actual\u001b[39;00m\n\u001b[32m      5\u001b[39m failure_message = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExpected \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpected_output\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, but found \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mactual_output\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m  \u001b[38;5;66;03m# Shows if actual and expected are not equal\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m expected_output == actual_output, failure_message\n",
      "\u001b[31mAssertionError\u001b[39m: Expected 25.8, but found 25.799999999999997"
     ]
    }
   ],
   "source": [
    "expected_output = 25.8\n",
    "actual_output = calculate_total_cost(number_of_items=3, cost_per_item=8.6)\n",
    "\n",
    "# Test that the expected equals actual\n",
    "failure_message = f\"Expected {expected_output}, but found {actual_output}\"  # Shows if actual and expected are not equal\n",
    "assert expected_output == actual_output, failure_message"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78400f56",
   "metadata": {},
   "source": [
    "We were very unlucky and encountered an issue with rounding, so below we tweak our function to handle this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6e091bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed the function via explicit rounding to 2 decminal places.\n",
    "def calculate_total_cost_2dp(number_of_items, cost_per_item):\n",
    "    total_cost = calculate_total_cost(number_of_items, cost_per_item)\n",
    "    return round(total_cost, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ee134f",
   "metadata": {},
   "source": [
    "Now we try again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c021ce97",
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_output = 25.8\n",
    "actual_output = calculate_total_cost_2dp(number_of_items=3, cost_per_item=8.6)  # New function with rounding\n",
    "\n",
    "# Test that the expected equals actual\n",
    "failure_message = f\"Expected {expected_output}, but found {actual_output}\"  # Shows if actual and expected are not equal\n",
    "assert expected_output == actual_output, failure_message"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6059083f",
   "metadata": {},
   "source": [
    "It works now! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc455521",
   "metadata": {},
   "source": [
    "Let's also test the second case from the table in the first part:\n",
    "\n",
    "| Inputs (Items, Cost per item) | Expected Output | Actual Output | Type         | Test Result |\n",
    "|------------------------------|-----------------|---------------|--------------|-------------|\n",
    "| (0, £9.35)                   | £0.00           | £0.00         | Edge         | Pass        |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6de62cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_output = 0.\n",
    "actual_output = calculate_total_cost_2dp(number_of_items=0, cost_per_item=9.35)\n",
    "\n",
    "# Test that the expected equals actual\n",
    "failure_message = f\"Expected {expected_output}, but found {actual_output}\"  # Shows if actual and expected are not equal\n",
    "assert expected_output == actual_output, failure_message"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75b4cfd",
   "metadata": {},
   "source": [
    "It works! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cabe2dff",
   "metadata": {},
   "source": [
    "#### Error Catching\n",
    "\n",
    "Sometimes we encounter errors in our code - which might happen by design or in unexpected circumstances. \n",
    "It's useful to understand how to catch them using a 'try-catch' block:\n",
    "```python\n",
    "# Try to catch the error\n",
    "try:\n",
    "    ... # Some code that might error\n",
    "except Exception:\n",
    "    ... # What to do if there is an error\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f45071",
   "metadata": {},
   "source": [
    "Let's design a test to catch the error we expect from the third case in the previous table:\n",
    "\n",
    "| Inputs (Items, Cost per item) | Expected Output | Actual Output | Type         | Test Result |\n",
    "|------------------------------|-----------------|---------------|--------------|-------------|\n",
    "| (-2, £7.20)                  | Error           | Error         | Error        | Pass        |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "811366cd",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Expected an error",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m     did_error = \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# Flag that we caught the error\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Did we see the error?\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m did_error, \u001b[33m\"\u001b[39m\u001b[33mExpected an error\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mAssertionError\u001b[39m: Expected an error"
     ]
    }
   ],
   "source": [
    "did_error = False  # A flag to see if we caught an error.\n",
    "\n",
    "# Try to catch the error\n",
    "try:\n",
    "    calculate_total_cost_2dp(number_of_items=-2, cost_per_item=7.2)\n",
    "except Exception:\n",
    "    did_error = True  # Flag that we caught the error\n",
    "\n",
    "# Did we see the error?\n",
    "assert did_error, \"Expected an error\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a939ffd",
   "metadata": {},
   "source": [
    "We didn't see an error, so we should tweak our function again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f5c5c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Newly tweaked function to validate the inputs and raise an error if there is an issue.\n",
    "def calculate_total_cost_2dp_with_validation(number_of_items, cost_per_item):\n",
    "    if number_of_items < 0:\n",
    "        raise ValueError(\"`number_of_items` must be positive\")\n",
    "    elif cost_per_item < 0:\n",
    "        raise ValueError(\"`cost_per_item` must be positive\")\n",
    "    else:\n",
    "        # Could also validate that the number of items is an integer (or sanitize it with rounding)\n",
    "        total_cost = calculate_total_cost_2dp(number_of_items, cost_per_item)\n",
    "        return total_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5cbe9c",
   "metadata": {},
   "source": [
    "Now let's try again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "793c00d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "did_error = False  # A flag to see if we caught an error.\n",
    "\n",
    "# Try to catch the error\n",
    "try:\n",
    "    calculate_total_cost_2dp_with_validation(number_of_items=-2, cost_per_item=7.2)\n",
    "except Exception:\n",
    "    did_error = True  # Flag that we caught the error\n",
    "\n",
    "# Did we see the error?\n",
    "assert did_error, \"Expected an error\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7eff5e",
   "metadata": {},
   "source": [
    "Success! Our function raised an error when the number of items was negative."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb4a21e",
   "metadata": {},
   "source": [
    "### Python Testing Frameworks\n",
    "As tests grow in number and complexity, it is often helpful to move from simple assert statements to a 'testing framework', which supports development and maintenance of suites of tests.\n",
    "\n",
    "There are a range of frameworks available as Python packages to support testing - some commonly used ones include:\n",
    "- [unittest](https://docs.python.org/3/library/unittest.html): Built-in Python testing framework. Although it's built-in, unittest can be tricky to use due to the boilerplate and implementation via classes.\n",
    "- [pytest](https://docs.pytest.org/en/stable/): Popular third-party framework with a simple syntax, which is fairly straightforward to use and has lots of helpful features. The examples below will use Pytest.\n",
    "- [doctest](https://docs.python.org/3/library/doctest.html#module-doctest)`: Tests embedded in documentation. This is also built in, but similar to unittest can be a little tricky to configure and use. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8985655",
   "metadata": {},
   "source": [
    "Below we will use pytest to develop and run our tests."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6bcb58",
   "metadata": {},
   "source": [
    "#### Setup\n",
    "Install and import the packages we need. \n",
    "\n",
    "Python testing frameworks are generally designed to work on .py files, so here we'll use pytest wrapper called `ipytest` which let's us run pytest tests inside a notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d4247cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipytest in c:\\users\\alijo\\repos\\coffee-and-coding\\2025-06-25 introduction to unit testing in python\\.venv\\lib\\site-packages (0.14.2)\n",
      "Requirement already satisfied: ipython in c:\\users\\alijo\\repos\\coffee-and-coding\\2025-06-25 introduction to unit testing in python\\.venv\\lib\\site-packages (from ipytest) (9.3.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\alijo\\repos\\coffee-and-coding\\2025-06-25 introduction to unit testing in python\\.venv\\lib\\site-packages (from ipytest) (25.0)\n",
      "Requirement already satisfied: pytest>=5.4 in c:\\users\\alijo\\repos\\coffee-and-coding\\2025-06-25 introduction to unit testing in python\\.venv\\lib\\site-packages (from ipytest) (8.4.1)\n",
      "Requirement already satisfied: colorama>=0.4 in c:\\users\\alijo\\repos\\coffee-and-coding\\2025-06-25 introduction to unit testing in python\\.venv\\lib\\site-packages (from pytest>=5.4->ipytest) (0.4.6)\n",
      "Requirement already satisfied: iniconfig>=1 in c:\\users\\alijo\\repos\\coffee-and-coding\\2025-06-25 introduction to unit testing in python\\.venv\\lib\\site-packages (from pytest>=5.4->ipytest) (2.1.0)\n",
      "Requirement already satisfied: pluggy<2,>=1.5 in c:\\users\\alijo\\repos\\coffee-and-coding\\2025-06-25 introduction to unit testing in python\\.venv\\lib\\site-packages (from pytest>=5.4->ipytest) (1.6.0)\n",
      "Requirement already satisfied: pygments>=2.7.2 in c:\\users\\alijo\\repos\\coffee-and-coding\\2025-06-25 introduction to unit testing in python\\.venv\\lib\\site-packages (from pytest>=5.4->ipytest) (2.19.2)\n",
      "Requirement already satisfied: decorator in c:\\users\\alijo\\repos\\coffee-and-coding\\2025-06-25 introduction to unit testing in python\\.venv\\lib\\site-packages (from ipython->ipytest) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers in c:\\users\\alijo\\repos\\coffee-and-coding\\2025-06-25 introduction to unit testing in python\\.venv\\lib\\site-packages (from ipython->ipytest) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\alijo\\repos\\coffee-and-coding\\2025-06-25 introduction to unit testing in python\\.venv\\lib\\site-packages (from ipython->ipytest) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\alijo\\repos\\coffee-and-coding\\2025-06-25 introduction to unit testing in python\\.venv\\lib\\site-packages (from ipython->ipytest) (0.1.7)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in c:\\users\\alijo\\repos\\coffee-and-coding\\2025-06-25 introduction to unit testing in python\\.venv\\lib\\site-packages (from ipython->ipytest) (3.0.51)\n",
      "Requirement already satisfied: stack_data in c:\\users\\alijo\\repos\\coffee-and-coding\\2025-06-25 introduction to unit testing in python\\.venv\\lib\\site-packages (from ipython->ipytest) (0.6.3)\n",
      "Requirement already satisfied: traitlets>=5.13.0 in c:\\users\\alijo\\repos\\coffee-and-coding\\2025-06-25 introduction to unit testing in python\\.venv\\lib\\site-packages (from ipython->ipytest) (5.14.3)\n",
      "Requirement already satisfied: typing_extensions>=4.6 in c:\\users\\alijo\\repos\\coffee-and-coding\\2025-06-25 introduction to unit testing in python\\.venv\\lib\\site-packages (from ipython->ipytest) (4.14.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in c:\\users\\alijo\\repos\\coffee-and-coding\\2025-06-25 introduction to unit testing in python\\.venv\\lib\\site-packages (from jedi>=0.16->ipython->ipytest) (0.8.4)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\alijo\\repos\\coffee-and-coding\\2025-06-25 introduction to unit testing in python\\.venv\\lib\\site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython->ipytest) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\alijo\\repos\\coffee-and-coding\\2025-06-25 introduction to unit testing in python\\.venv\\lib\\site-packages (from stack_data->ipython->ipytest) (2.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\alijo\\repos\\coffee-and-coding\\2025-06-25 introduction to unit testing in python\\.venv\\lib\\site-packages (from stack_data->ipython->ipytest) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\alijo\\repos\\coffee-and-coding\\2025-06-25 introduction to unit testing in python\\.venv\\lib\\site-packages (from stack_data->ipython->ipytest) (0.2.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install ipytest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a3686b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "import ipytest\n",
    "\n",
    "ipytest.autoconfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ba003c",
   "metadata": {},
   "source": [
    "#### Pytest: Basic Usage\n",
    "Let's rewrite the first test case defined above in pytest:\n",
    "```python\n",
    "# Previously\n",
    "expected_output = 25.8\n",
    "actual_output = calculate_total_cost(number_of_items=3, cost_per_item=8.6)\n",
    "\n",
    "# Test that the expected equals actual\n",
    "failure_message = f\"Expected {expected_output}, but found {actual_output}\"  # Shows if actual and expected are not equal\n",
    "assert expected_output == actual_output, failure_message\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "03fb3985",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m.\u001b[0m\u001b[32m                                                                                            [100%]\u001b[0m\n",
      "\u001b[32m\u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.02s\u001b[0m\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ExitCode.OK: 0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ipytest.clean()  # Reset our test suite\n",
    "\n",
    "\n",
    "# Using Pytest\n",
    "def test_case1():\n",
    "    expected_output = 25.8\n",
    "    actual_output = calculate_total_cost_2dp(number_of_items=3, cost_per_item=8.6)\n",
    "\n",
    "    # Test that the expected equals actual\n",
    "    failure_message = f\"Expected {expected_output}, but found {actual_output}\"  # Shows if actual and expected are not equal\n",
    "    assert expected_output == actual_output, failure_message\n",
    "\n",
    "\n",
    "ipytest.run()  # Run the tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074d01ee",
   "metadata": {},
   "source": [
    "Note that the syntax inside the test is the same as before: we run the function with an assert statement on the result.\n",
    "\n",
    "However, by putting the test inside a function using Pytest, we gain a number of benefits:\n",
    "1. It is more obvious that the code we have written is a test (Pytest recognises `test_` functions as tests)\n",
    "2. We have enhanced output and logging for our tests, which shows passes and failures and helps us diagnose issues (see below for examples)\n",
    "3. Pytest will also run every test, regardless of pass or failure, whereas simple `assert` statements will stop at the first failure (again, this will be clearer below)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e206f67",
   "metadata": {},
   "source": [
    "### Pytest: Multiple Tests\n",
    "As mentioned above, we can write multiple tests in Pytest and run them together. \n",
    "They will all run, regardless of passes or failures. \n",
    "This let's us see all the issues with our code at once and fix it, rather than running and rerunning as we work through them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a70baaf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[31mF\u001b[0m\u001b[31m                                                                                          [100%]\u001b[0m\n",
      "============================================ FAILURES =============================================\n",
      "\u001b[31m\u001b[1m___________________________________________ test_case3 ____________________________________________\u001b[0m\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_case3\u001b[39;49;00m():\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# Pytest makes checking for errors easier - we don't need to track the\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# 'did_raise' flag ourself, it can do it for us!\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mwith\u001b[39;49;00m pytest.raises(\u001b[96mValueError\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       Failed: DID NOT RAISE <class 'ValueError'>\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mC:\\Users\\ALIJO\\AppData\\Local\\Temp\\ipykernel_4896\\2933397068.py\u001b[0m:25: Failed\n",
      "\u001b[36m\u001b[1m===================================== short test summary info =====================================\u001b[0m\n",
      "\u001b[31mFAILED\u001b[0m t_93529f45e19f4cf78e770740a316ad74.py::\u001b[1mtest_case3\u001b[0m - Failed: DID NOT RAISE <class 'ValueError'>\n",
      "\u001b[31m\u001b[31m\u001b[1m1 failed\u001b[0m, \u001b[32m2 passed\u001b[0m\u001b[31m in 0.09s\u001b[0m\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ExitCode.TESTS_FAILED: 1>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ipytest.clean()  # Reset our test suite\n",
    "\n",
    "\n",
    "def test_case1():\n",
    "    expected_output = 25.8\n",
    "    actual_output = calculate_total_cost_2dp(number_of_items=3, cost_per_item=8.6)\n",
    "\n",
    "    # Test that the expected equals actual\n",
    "    failure_message = f\"Expected {expected_output}, but found {actual_output}\"  # Shows if actual and expected are not equal\n",
    "    assert expected_output == actual_output, failure_message\n",
    "\n",
    "\n",
    "def test_case2():\n",
    "    expected_output = 0.\n",
    "    actual_output = calculate_total_cost_2dp(number_of_items=0, cost_per_item=9.35)\n",
    "\n",
    "    # Test that the expected equals actual\n",
    "    failure_message = f\"Expected {expected_output}, but found {actual_output}\"  # Shows if actual and expected are not equal\n",
    "    assert expected_output == actual_output, failure_message\n",
    "\n",
    "\n",
    "def test_case3():\n",
    "    # Pytest makes checking for errors easier - we don't need to track the \n",
    "    # 'did_raise' flag ourself, it can do it for us!\n",
    "    with pytest.raises(ValueError):\n",
    "        calculate_total_cost_2dp(number_of_items=-2, cost_per_item=7.2)\n",
    "\n",
    "\n",
    "ipytest.run()  # Run the tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96533e48",
   "metadata": {},
   "source": [
    "Now we can see that we had 2 passes and 1 failure. \n",
    "We can also see that it was the third test that failed, because we didn't get an error.\n",
    "This helps us understand what we need to do to fix it!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58ad1a5",
   "metadata": {},
   "source": [
    "### Pytest: Parameterising Tests\n",
    "The final feature of Pytest to show in this session is parameterising tests. \n",
    "You may have noticed that we use a similar set of steps each time we run a test: run a function with some parameters and check the result. \n",
    "Rather than writing each test individually with lots of repitition, we can use the `pytest.mark.parametrize` function decorator, which allows us to pass sets of parameters into a function that each get run as an isolated test case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "33d9ba18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[31mF\u001b[0m\u001b[31mF\u001b[0m\u001b[31m                                                                                       [100%]\u001b[0m\n",
      "============================================ FAILURES =============================================\n",
      "\u001b[31m\u001b[1m_________________________________ test_expected_errors[None-5.3] __________________________________\u001b[0m\n",
      "\n",
      "number_of_items = None, cost_per_item = 5.3\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[90m\u001b[39;49;00m\n",
      "        (\u001b[33m\"\u001b[39;49;00m\u001b[33mnumber_of_items\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcost_per_item\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m),  \u001b[90m# The names of our test function arguments\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# Each element in the list below defines an isolated set of parameters to run a test\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        [\u001b[90m\u001b[39;49;00m\n",
      "            (-\u001b[94m2\u001b[39;49;00m, \u001b[94m7.2\u001b[39;49;00m),  \u001b[90m# (number_of_items, cost_per_item)\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "            (\u001b[94mNone\u001b[39;49;00m, \u001b[94m5.3\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "            (\u001b[94m1\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mten pounds 20 pence\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "        ]\u001b[90m\u001b[39;49;00m\n",
      "    )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_expected_errors\u001b[39;49;00m(number_of_items, cost_per_item):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mwith\u001b[39;49;00m pytest.raises(\u001b[96mValueError\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n",
      ">           calculate_total_cost_2dp_with_validation(number_of_items, cost_per_item)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "\u001b[1m\u001b[31mC:\\Users\\ALIJO\\AppData\\Local\\Temp\\ipykernel_4896\\4205113075.py\u001b[0m:35: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "\n",
      "number_of_items = None, cost_per_item = 5.3\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mcalculate_total_cost_2dp_with_validation\u001b[39;49;00m(number_of_items, cost_per_item):\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mif\u001b[39;49;00m number_of_items < \u001b[94m0\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "           ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       TypeError: '<' not supported between instances of 'NoneType' and 'int'\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mC:\\Users\\ALIJO\\AppData\\Local\\Temp\\ipykernel_4896\\2037280060.py\u001b[0m:3: TypeError\n",
      "\u001b[31m\u001b[1m___________________________ test_expected_errors[1-ten pounds 20 pence] ___________________________\u001b[0m\n",
      "\n",
      "number_of_items = 1, cost_per_item = 'ten pounds 20 pence'\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[90m\u001b[39;49;00m\n",
      "        (\u001b[33m\"\u001b[39;49;00m\u001b[33mnumber_of_items\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcost_per_item\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m),  \u001b[90m# The names of our test function arguments\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# Each element in the list below defines an isolated set of parameters to run a test\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        [\u001b[90m\u001b[39;49;00m\n",
      "            (-\u001b[94m2\u001b[39;49;00m, \u001b[94m7.2\u001b[39;49;00m),  \u001b[90m# (number_of_items, cost_per_item)\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "            (\u001b[94mNone\u001b[39;49;00m, \u001b[94m5.3\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "            (\u001b[94m1\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mten pounds 20 pence\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "        ]\u001b[90m\u001b[39;49;00m\n",
      "    )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_expected_errors\u001b[39;49;00m(number_of_items, cost_per_item):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mwith\u001b[39;49;00m pytest.raises(\u001b[96mValueError\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n",
      ">           calculate_total_cost_2dp_with_validation(number_of_items, cost_per_item)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "\u001b[1m\u001b[31mC:\\Users\\ALIJO\\AppData\\Local\\Temp\\ipykernel_4896\\4205113075.py\u001b[0m:35: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "\n",
      "number_of_items = 1, cost_per_item = 'ten pounds 20 pence'\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mcalculate_total_cost_2dp_with_validation\u001b[39;49;00m(number_of_items, cost_per_item):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m number_of_items < \u001b[94m0\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mraise\u001b[39;49;00m \u001b[96mValueError\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33m`number_of_items` must be positive\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94melif\u001b[39;49;00m cost_per_item < \u001b[94m0\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "             ^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       TypeError: '<' not supported between instances of 'str' and 'int'\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mC:\\Users\\ALIJO\\AppData\\Local\\Temp\\ipykernel_4896\\2037280060.py\u001b[0m:5: TypeError\n",
      "\u001b[36m\u001b[1m===================================== short test summary info =====================================\u001b[0m\n",
      "\u001b[31mFAILED\u001b[0m t_93529f45e19f4cf78e770740a316ad74.py::\u001b[1mtest_expected_errors[None-5.3]\u001b[0m - TypeError: '<' not supported between instances of 'NoneType' and 'int'\n",
      "\u001b[31mFAILED\u001b[0m t_93529f45e19f4cf78e770740a316ad74.py::\u001b[1mtest_expected_errors[1-ten pounds 20 pence]\u001b[0m - TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\u001b[31m\u001b[31m\u001b[1m2 failed\u001b[0m, \u001b[32m4 passed\u001b[0m\u001b[31m in 0.09s\u001b[0m\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ExitCode.TESTS_FAILED: 1>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ipytest.clean()  # Reset our test suite\n",
    "\n",
    "\n",
    "# Test function for cases 1, 2 and 4, which are all along the 'happy' path.\n",
    "@pytest.mark.parametrize(\n",
    "    (\"number_of_items\", \"cost_per_item\", \"expected_output\"),  # The names of our test function arguments\n",
    "    # Each element in the list below defines an isolated set of parameters to run a test \n",
    "    [\n",
    "        (3, 8.6, 25.8),  # (number_of_items, cost_per_item, expected_output)\n",
    "        (0, 9.35, 0),\n",
    "        (10, 8.6, 86),\n",
    "    ]\n",
    ")\n",
    "def test_expected_outputs(number_of_items, cost_per_item, expected_output):\n",
    "    actual_output = calculate_total_cost_2dp_with_validation(number_of_items, cost_per_item)\n",
    "\n",
    "    # Test that the expected equals actual\n",
    "    failure_message = f\"Expected {expected_output}, but found {actual_output}\"  # Shows if actual and expected are not equal\n",
    "    assert expected_output == actual_output, failure_message\n",
    "\n",
    "    \n",
    "# A similar test function, but for the cases where we expect an error\n",
    "# since this has a slightly different structure inside the test\n",
    "@pytest.mark.parametrize(\n",
    "    (\"number_of_items\", \"cost_per_item\"),  # The names of our test function arguments\n",
    "    # Each element in the list below defines an isolated set of parameters to run a test \n",
    "    [\n",
    "        (-2, 7.2),  # (number_of_items, cost_per_item)\n",
    "        (None, 5.3),\n",
    "        (1, 'ten pounds 20 pence'),\n",
    "    ]\n",
    ")\n",
    "def test_expected_errors(number_of_items, cost_per_item):\n",
    "    with pytest.raises(ValueError):\n",
    "        calculate_total_cost_2dp_with_validation(number_of_items, cost_per_item)\n",
    "\n",
    "\n",
    "ipytest.run()  # Run the tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13216f75",
   "metadata": {},
   "source": [
    "Now we can see from the output that we had 2 failures and 4 passes across our 6 test cases. \n",
    "Even better, we can see which cases failed and why! \n",
    "This means we can fix them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ca6fb909",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final revised version of our function with validation and type casting\n",
    "def calculate_total_cost_2dp_with_validation_and_casting(number_of_items, cost_per_item):\n",
    "    if number_of_items is None:\n",
    "        raise ValueError(\"`number_of_items` must not be None\")\n",
    "    elif cost_per_item is None:\n",
    "        raise ValueError(\"`cost_per_item` must not be None\")\n",
    "    else:\n",
    "        number_of_items = int(number_of_items)  # Make sure number_of_items is an integer\n",
    "        cost_per_item = float(cost_per_item)  # # Make sure cost_per_items is a float\n",
    "        total_cost = calculate_total_cost_2dp_with_validation(number_of_items, cost_per_item)\n",
    "        return total_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605ab319",
   "metadata": {},
   "source": [
    "Finally we can rerun our test suite:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9e8616f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                                                       [100%]\u001b[0m\n",
      "\u001b[32m\u001b[32m\u001b[1m6 passed\u001b[0m\u001b[32m in 0.03s\u001b[0m\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ExitCode.OK: 0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ipytest.clean()  # Reset our test suite\n",
    "\n",
    "\n",
    "# Test function for cases 1, 2 and 4, which are all along the 'happy' path.\n",
    "@pytest.mark.parametrize(\n",
    "    (\"number_of_items\", \"cost_per_item\", \"expected_output\"),  # The names of our test function arguments\n",
    "    # Each element in the list below defines an isolated set of parameters to run a test \n",
    "    [\n",
    "        (3, 8.6, 25.8),  # (number_of_items, cost_per_item, expected_output)\n",
    "        (0, 9.35, 0),\n",
    "        (10, 8.6, 86),\n",
    "    ]\n",
    ")\n",
    "def test_expected_outputs(number_of_items, cost_per_item, expected_output):\n",
    "    actual_output = calculate_total_cost_2dp_with_validation_and_casting(number_of_items, cost_per_item)\n",
    "\n",
    "    # Test that the expected equals actual\n",
    "    failure_message = f\"Expected {expected_output}, but found {actual_output}\"  # Shows if actual and expected are not equal\n",
    "    assert expected_output == actual_output, failure_message\n",
    "\n",
    "    \n",
    "# A similar test function, but for the cases where we expect an error\n",
    "# since this has a slightly different structure inside the test\n",
    "@pytest.mark.parametrize(\n",
    "    (\"number_of_items\", \"cost_per_item\"),  # The names of our test function arguments\n",
    "    # Each element in the list below defines an isolated set of parameters to run a test \n",
    "    [\n",
    "        (-2, 7.2),  # (number_of_items, cost_per_item)\n",
    "        (None, 5.3),\n",
    "        (1, 'ten pounds 20 pence'),\n",
    "    ]\n",
    ")\n",
    "def test_expected_errors(number_of_items, cost_per_item):\n",
    "    with pytest.raises(ValueError):\n",
    "        calculate_total_cost_2dp_with_validation_and_casting(number_of_items, cost_per_item)\n",
    "\n",
    "\n",
    "ipytest.run()  # Run the tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36969cd4",
   "metadata": {},
   "source": [
    "Success! We've now got a function that works as described in the original table in part 1 :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ddc3e4",
   "metadata": {},
   "source": [
    "### Bonus: Testing Pandas DataFrames\n",
    "Finally, this bonus section looks at how we can apply some of what we've learned above when we are using Pandas DataFrames.\n",
    "This will probably feel more applicable in most analytical workflows where we are dealing with full datasets, rather than individual inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e33c0ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-2.3.0-cp311-cp311-win_amd64.whl.metadata (19 kB)\n",
      "Collecting numpy>=1.23.2 (from pandas)\n",
      "  Downloading numpy-2.3.1-cp311-cp311-win_amd64.whl.metadata (60 kB)\n",
      "     ---------------------------------------- 0.0/60.9 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/60.9 kB ? eta -:--:--\n",
      "     ------------------------- ------------ 41.0/60.9 kB 667.8 kB/s eta 0:00:01\n",
      "     -------------------------------------- 60.9/60.9 kB 649.3 kB/s eta 0:00:00\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\alijo\\repos\\coffee-and-coding\\2025-06-25 introduction to unit testing in python\\.venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\alijo\\repos\\coffee-and-coding\\2025-06-25 introduction to unit testing in python\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Downloading pandas-2.3.0-cp311-cp311-win_amd64.whl (11.1 MB)\n",
      "   ---------------------------------------- 0.0/11.1 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.2/11.1 MB 4.1 MB/s eta 0:00:03\n",
      "   - -------------------------------------- 0.5/11.1 MB 6.1 MB/s eta 0:00:02\n",
      "   --- ------------------------------------ 0.9/11.1 MB 6.5 MB/s eta 0:00:02\n",
      "   ---- ----------------------------------- 1.3/11.1 MB 6.9 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 2.0/11.1 MB 8.3 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 2.6/11.1 MB 9.1 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 3.2/11.1 MB 9.7 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 3.8/11.1 MB 10.2 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 4.5/11.1 MB 10.5 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 5.1/11.1 MB 10.9 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 5.8/11.1 MB 11.2 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 6.4/11.1 MB 11.0 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 7.0/11.1 MB 11.5 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 7.7/11.1 MB 11.4 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 8.5/11.1 MB 12.1 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 9.6/11.1 MB 12.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.1/11.1 MB 15.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.1/11.1 MB 14.5 MB/s eta 0:00:00\n",
      "Downloading numpy-2.3.1-cp311-cp311-win_amd64.whl (13.0 MB)\n",
      "   ---------------------------------------- 0.0/13.0 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 1.3/13.0 MB 39.4 MB/s eta 0:00:01\n",
      "   -------- ------------------------------- 2.7/13.0 MB 28.1 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 3.6/13.0 MB 25.5 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 4.8/13.0 MB 28.0 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 6.2/13.0 MB 28.4 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 7.3/13.0 MB 27.3 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 8.7/13.0 MB 26.4 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 9.9/13.0 MB 26.4 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 10.8/13.0 MB 25.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 12.0/13.0 MB 26.2 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 12.5/13.0 MB 26.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  13.0/13.0 MB 23.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 13.0/13.0 MB 21.1 MB/s eta 0:00:00\n",
      "Downloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "   ---------------------------------------- 0.0/509.2 kB ? eta -:--:--\n",
      "   --------------------------------------- 509.2/509.2 kB 15.6 MB/s eta 0:00:00\n",
      "Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "   ---------------------------------------- 0.0/347.8 kB ? eta -:--:--\n",
      "   --------------------------------------- 347.8/347.8 kB 10.9 MB/s eta 0:00:00\n",
      "Installing collected packages: pytz, tzdata, numpy, pandas\n",
      "Successfully installed numpy-2.3.1 pandas-2.3.0 pytz-2025.2 tzdata-2025.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82c184e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas.testing import assert_frame_equal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e622cb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame version of our function from above\n",
    "def calculate_total_cost_pandas(df_input):\n",
    "    df_result = df_input.copy()  # Copy to avoid modifying in place\n",
    "    df_result[\"total_cost\"] = df_input[\"number_of_items\"] * df_input[\"cost_per_item\"]\n",
    "    return df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3cb51bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>number_of_items</th>\n",
       "      <th>cost_per_item</th>\n",
       "      <th>total_cost</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>8.60</td>\n",
       "      <td>25.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>9.35</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>8.60</td>\n",
       "      <td>86.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   number_of_items  cost_per_item  total_cost\n",
       "0                3           8.60        25.8\n",
       "1                0           9.35         0.0\n",
       "2               10           8.60        86.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Data we expect to see from running our calculation\n",
    "expected_data = [\n",
    "    (3, 8.6, 25.8),  # (number_of_items, cost_per_item, total_cost)\n",
    "    (0, 9.35, 0),\n",
    "    (10, 8.6, 86),\n",
    "]\n",
    "expected_columns = [\"number_of_items\", \"cost_per_item\", \"total_cost\"]\n",
    "df_expected = pd.DataFrame(expected_data, columns=expected_columns)\n",
    "display(df_expected)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "674b2c6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   number_of_items  cost_per_item\n",
      "0                3           8.60\n",
      "1                0           9.35\n",
      "2               10           8.60\n"
     ]
    }
   ],
   "source": [
    "# Remove the result column before inputting\n",
    "df_input = df_expected.drop(labels=[\"total_cost\"], axis=\"columns\")\n",
    "print(df_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c88559c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the function to get the result\n",
    "df_actual = calculate_total_cost_pandas(df_input)\n",
    "\n",
    "# Check that we got the expected result\n",
    "assert_frame_equal(df_actual, df_expected)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e21243",
   "metadata": {},
   "source": [
    "We see that the function returns the expected result. \n",
    "The next step would be to wrap up our assert into a pytest function, following the patterns shown in the sections above. \n",
    "But we'll leave it there for now! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018d166e",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "We've discussed:\n",
    "- What unit testing is and why it's important\n",
    "- Some Best Practice considerations\n",
    "- Unit testing examples using Python and Pytest\n",
    "\n",
    "We've not considered:\n",
    "- More advanced features of pytest, including fixtures, mocking and configuration: see [Effective Python Testing With pytest](https://realpython.com/pytest-python-testing/) for further guidance and examples.\n",
    "- Testing frameworks for other tools and technologies - the following may be helpful:\n",
    "    - R:\n",
    "        - [testthat](https://testthat.r-lib.org/): R testing framework\n",
    "        - [testthat: Get Started with Testing](https://journal.r-project.org/archive/2011-1/RJournal_2011-1_Wickham.pdf): Guidance on using 'testthat' by Hadley Wickham\n",
    "    - Alteryx: \n",
    "        - [Test Tool](https://help.alteryx.com/current/en/designer/tools/developer/test-tool.html#idp388647): Tool to help verify data or processes in a workflow. \n",
    "        - [Alteryx Test Tool Demonstration (Youtube)](https://youtu.be/Wgz7HSzXWJw?si=aAvOr6fwhxwINkwq): Short video demo of the Test Tool usage.\n",
    "    - Spark:\n",
    "        - [Testing PySpark](https://spark.apache.org/docs/latest/api/python/getting_started/testing_pyspark.html): Apache guidance for testing PySpark Code\n",
    "- Testing Pandas DataFrames (which is important for testing analytical pipelines): see [Pandas Testing](https://pandas.pydata.org/pandas-docs/stable/reference/testing.html) for functions to help with this.\n",
    "\n",
    "Please get in touch if you have any questions or want to discuss testing further! :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
