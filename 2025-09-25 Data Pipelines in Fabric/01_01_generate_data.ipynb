{"cells":[{"cell_type":"markdown","source":["# 01 (01): Generate Data\n","\n","In this notebook, we generate some fake medical datasets to work with. One is fake patient data, containing personal details such as the name and address.\n","\n","The other dataset contains information about appointments they have attended, that we aggregate later."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"2fd4ed8a-89da-431d-99aa-deee7c68439b"},{"cell_type":"code","source":["# Install the faker package (not included with Fabric by default)\n","!pip install faker"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"7b610c24-a799-47ab-b509-0921bcf6c914"},{"cell_type":"code","source":["# Import packages\n","from datetime import datetime\n","from faker import Faker\n","\n","import pandas as pd\n","import numpy as np\n","import random"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"6f8a17b4-12d9-404e-83a8-c5daeaac5e15"},{"cell_type":"code","source":["# Initialize Faker with UK locale\n","fake = Faker('en_GB')\n","\n","# Define number of records and random seed\n","num_records = 20000\n","SEED = 42\n","\n","# Set random seeds to ensure we always get the same data\n","random.seed(SEED)\n","np.random.seed(SEED)\n","fake.seed_instance(SEED)"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"6a61cec6-e9e4-4a02-94f8-cdcfbb509fc2"},{"cell_type":"markdown","source":["## Generate fake patient data"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"00674c59-7f68-431d-b7fa-558fbedeceab"},{"cell_type":"code","source":["# Generate synthetic medical dataset with intentional errors\n","data = []\n","for i in range(1, num_records + 1):\n","    # Introduce unrealistic dates of birth\n","    dob = fake.date_of_birth(minimum_age=0, maximum_age=90)\n","    if random.random() < 0.005:\n","        dob = datetime.strptime(f\"{random.randint(2025, 2030)}-{random.randint(1,12):02d}-{random.randint(1,28):02d}\", \"%Y-%m-%d\").date()\n","    dob = pd.to_datetime(dob)\n","\n","    # Introduce missing values randomly\n","    data.append({\n","        'patient_id': i,\n","        'name': fake.name(),\n","        'date_of_birth': dob,\n","        'address': fake.address().replace('\\n', ', ') if random.random() > 0.01 else '',\n","        'phone_number': fake.phone_number() if random.random() > 0.01 else 'N/A',\n","        'is_public_patient': random.choice([1, 0]) \n","    })\n","\n","# Save to data frame\n","df = pd.DataFrame(data)\n","df.head(2)"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"62c1c30e-b11a-4393-bde5-9d0214280eca"},{"cell_type":"markdown","source":["## Generate fake appointment data"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"e9d0b4bd-01b4-4b7c-95a2-9ce7ba0bcc77"},{"cell_type":"code","source":["min_appointments, max_appointments= 0, 5\n","\n","# Generate appointment history\n","appointment_data = []\n","for patient_id in range(1, num_records + 1):\n","    num_appointments = random.randint(min_appointments, max_appointments)\n","    \n","    # Assign 1 to 3 doctors per patient\n","    doctors = [fake.name() for _ in range(random.randint(1, 3))]\n","    \n","    for _ in range(num_appointments):\n","        appointment_date = fake.date_between(start_date='-2y', end_date='today')\n","        doctor_seen = random.choice(doctors)\n","        \n","        appointment_data.append({\n","            'patient_id': patient_id,\n","            'appointment_date': appointment_date,\n","            'doctor_seen': doctor_seen\n","        })\n","\n","# Save to CSV\n","appointments_df = pd.DataFrame(appointment_data)\n","appointments_df.head(2)"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"17f9cdb9-e669-4619-a5bf-11fb6edeffbd"},{"cell_type":"markdown","source":["## Upload data to the lakehouse\n","\n","We will discuss this later. **For now, change the lakehouse path to your own.**\n","\n","To extract the lakehouse path:\n","\n","* navigate to your lakehouse,\n","* click on the three dots next to 'Tables',\n","* copy the ABFS path."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"8feadb30-2e34-420b-860d-dfef887b6a29"},{"cell_type":"code","source":["# Specify lakehouse path\n","abfs_path = 'abfss://490a35a8-ffa1-4c26-8ad2-f394ba2aaefd@onelake.dfs.fabric.microsoft.com/e5d53df5-727e-4244-8a8e-eb9d9b6ac78b/Tables'\n","\n","tables = {\n","    'personal_df': df,\n","    'appointment_df': appointments_df\n","}\n","\n","# Save the dataframes to the lakehouse\n","for table_name, table in tables.items():\n","    df_spark = spark.createDataFrame(table)\n","    (\n","    df_spark\n","        .write\n","        .mode('overwrite')\n","        .format('delta')\n","        .option('overwriteSchema', 'true')\n","        .save(f\"{abfs_path}/{table_name}\")\n","    )"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"8297a7f2-6673-4265-9890-80fb036106c6"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","display_name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{}}},"nbformat":4,"nbformat_minor":5}