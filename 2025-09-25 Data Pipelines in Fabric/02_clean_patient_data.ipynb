{"cells":[{"cell_type":"markdown","source":["# 02: Clean Patient Data\n","\n","Instead of running everything in one big notebook, it's best to split every process into individual notebooks so that if something goes wrong with our workflow, we can see where the error was and easily correct it.\n","\n","Therefore, this notebook is the first item that we will include within our data pipeline. \n","\n","We take the code from the exploration and add it here as a step in the pipeline."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"c7feba49-d3c9-45c6-bdcd-8d1b9223fdde"},{"cell_type":"markdown","source":["\n","The first step we apply here is by running the config file. You can see this notebook within the list of files. \n","\n","We need to copy the ABFS path again, navigate to the config, paste that in and come back!\n"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"4e68457b-5c9c-4e8a-84ee-fd56a033f584"},{"cell_type":"code","source":["%run 00_config"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"dd0b8d43-5fe5-4363-aa39-5a5e5ae275f2"},{"cell_type":"code","source":["config"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"69574c6c-4b7f-4b49-9840-d80b3837cf28"},{"cell_type":"markdown","source":["**Using this file means that if we ever change any values, such as the lakehouse name, we only need to change one file, rather than all files.**\n","\n","We now just run our script that we created previously and re-upload the lakehouse.\n","\n","Note the use of `f\"\"\"` in our script so that we can insert variables via `{...}`. "],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"572e5210-edd8-4aa0-90c0-932da8e55135"},{"cell_type":"code","source":["# Read the data from the lakehouse\n","df = spark.sql(f\"\"\"\n","SELECT\n","    patient_id,\n","    NULLIF(UPPER(REGEXP_EXTRACT(name, '^(Mrs|Mr|Ms|Dr|Prof|Rev|Sir|Madam)', 1)), '') AS title,\n","    UPPER(REGEXP_REPLACE(name, '^(Mrs|Mr|Ms|Dr|Prof|Rev|Sir|Madam)', '')) AS name,\n","    TO_DATE(date_of_birth) AS date_of_birth,\n","    NULLIF(UPPER(REGEXP_REPLACE(TRIM(CONCAT_WS(',', SLICE(SPLIT(address, ','), 1, SIZE(SPLIT(address, ',')) - 1))), '[^a-zA-Z0-9]', ' ')), '') AS address,\n","    NULLIF(TRIM(REGEXP_REPLACE(ELEMENT_AT(SPLIT(address, ','), -1), ' ', '')), '') AS postcode,\n","    NULLIF(REGEXP_REPLACE(phone_number, '(\\\\\\\\+44\\\\\\\\(0\\\\\\\\)|\\\\\\\\+44|\\\\\\\\)|\\\\\\\\()| ', ''), 'N/A') AS phone_number,\n","    is_public_patient\n","FROM \n","    {config['lakehouse_name']}.personal_df\n","WHERE\n","    date_of_birth <= current_date()\n","\"\"\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"7ad963e6-9e0e-48bf-a622-ae6660b5468a"},{"cell_type":"markdown","source":["NOTE: We don't need to specify the path here as it's in our config and don't need to convert to Spark dataframe."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"27d2fd74-ec69-40e9-b302-dc77991c443a"},{"cell_type":"code","source":["# Save the data to the lakehouse\n","(\n","df\n","    .write\n","    .mode('overwrite')\n","    .format('delta')\n","    .option('overwriteSchema', 'true')\n","    .save(f\"{config['lakehouse_path']}/personal_df_clean\")\n",")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"49c56564-128e-43e7-a540-81b43eea4162"}],"metadata":{"language_info":{"name":"python"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"microsoft":{"language":"python","ms_spell_check":{"ms_spell_check_language":"en"},"language_group":"synapse_pyspark"},"widgets":{},"kernel_info":{"name":"synapse_pyspark"},"nteract":{"version":"nteract-front-end@1.0.0"},"notebook_environment":{},"synapse_widget":{"version":"0.1","state":{}},"save_output":true,"spark_compute":{"compute_id":"/trident/default","session_options":{"enableDebugMode":false,"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{}}},"nbformat":4,"nbformat_minor":5}