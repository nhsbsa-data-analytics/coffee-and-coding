{"cells":[{"cell_type":"markdown","source":["# 01 (02): Explore Data\n","\n","In this notebook, we will explore the data to see which columns require cleaning, as well as potentially creating new columns.\n","\n","First, we import some packages and print the full datasets. This can be done by first adding the lakehouse on the left pane and dragging + dropping the table onto the notebook."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"90b8c104-464c-4d38-b574-56993c9bc2ea"},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import re"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"4918e357-727b-464d-9bdc-16a191a5f49b"},{"cell_type":"code","source":["# Read the data from the lakehouse\n","df = spark.sql(\"SELECT * FROM coffee_lakehouse.personal_df\")\n","display(df)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"a00f7537-2042-433c-b730-f10b1b72d14b"},{"cell_type":"markdown","source":["By viewing the data, there are Some initial things we can see:\n","\n","* The `name` contains titles.\n","* The `phone_number` contains +44 and some area codes.\n","\n","We will tackle these first."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"fb7a1b0d-fbea-498c-9682-bad1d644e655"},{"cell_type":"code","source":["# Read the data from the lakehouse\n","df_app = spark.sql(\"SELECT * FROM coffee_lakehouse.appointment_df\")\n","display(df_app)"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"506ae4ee-e398-4763-b4a8-65922bbdbd77"},{"cell_type":"markdown","source":["Similarly, the `doctor_seen` column also contains titles which we can extract."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"e161982a-bace-42bc-9a0a-f8e149cc6de4"},{"cell_type":"markdown","source":["## Patient Data Exploration\n","\n","We will start by exploring and cleaning the patient data first. \n","\n","There are 3 primary ways that we can work with our data inside notebooks. These are:\n","\n","* Pandas\n","    * Best for: Small data, quick analysis.\n","    * Pros: Easy, fast for small datasets and uses standard Python.\n","    * Cons: Doesn't scale well and not good for big data.\n","* PySpark\n","    * Best for: Big data, distributed computing.\n","    * Pros: Scales massively, fault-tolerant.\n","    * Cons: More complex setup and usage. \n","* Spark SQL\n","    * Best for: SQL-style queries on big data.\n","    * Pros: Allows usage of SQL code while using the power of Spark.\n","    * Cons: Less flexible than PySpark.\n","\n","In addition to this, as we'll see below, any combination of the three can be used in combination to achieve great things!\n"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"83c13a8a-1267-4353-967d-a51f25a567bd"},{"cell_type":"markdown","source":["### Pandas\n","\n","\n","To start us off, we will explore and transform the data using Pandas.\n","\n","First, because the dataframe by default is a Spark dataframe, we must convert it to Pandas."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"b438b547-e2bb-41fb-8e40-f105cde773ea"},{"cell_type":"code","source":["df_pandas = df.toPandas()"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"ab27d448-fd9a-4e1f-929e-ff8d66bfd3d5"},{"cell_type":"markdown","source":["First, we will tackle where the `name` contains titles and `phone_number` contains +44 and some area codes as we saw above.\n","\n","For the former, we extract the title into a new column which we can use later. For the latter, we just simply remove these."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"58068bfb-d632-4782-ab99-64ea71acaa99"},{"cell_type":"code","source":["# Extract titles\n","df_pandas['title'] = df_pandas['name'].str.extract(r'^(Mrs|Mr|Ms|Dr|Prof|Rev|Sir|Madam)')\n","\n","# Remove title from the original name\n","df_pandas['name'] = df_pandas['name'].str.replace(r'^(Mrs|Mr|Ms|Dr|Prof|Rev|Sir|Madam)', '', regex=True)\n","\n","# Remove +44 and brackets from phone numbers\n","df_pandas['phone_number'] = df_pandas['phone_number'].str.replace(r'(\\+44\\(0\\)|\\+44|\\)|\\()', '', regex=True)\n","\n","# Remove spaces from numbers\n","df_pandas['phone_number'] = df_pandas['phone_number'].str.replace(' ', '')"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"9702db0e-06f5-4358-a26c-1e3f6372a9d0"},{"cell_type":"markdown","source":["We then do a quick check to make sure these changes took effect."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"3c7117fc-bcea-41cd-b7d4-8063720bd292"},{"cell_type":"code","source":["df_pandas[~df_pandas['title'].isna()]"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"dc7af35d-0e9f-497c-a1ef-b600061c6989"},{"cell_type":"markdown","source":["We now quickly loop through all of the columns, performing a quick count per value to identify other anomalies. \n","\n","Ordering by count we see:\n","\n","* Address contains some blank values ('')\n","* Phone number contains some 'N/A' values.\n","\n","Order by the column, we also see:\n","\n","* There is some DOBs that are in the future"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"85d53a8a-36d8-4249-8a7c-261e73e2623c"},{"cell_type":"code","source":["for col_name in df.columns:\n","    print(col_name)\n","    display(df_pandas[col_name].value_counts().reset_index())"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"3d266a95-5026-473e-8ace-bb36fcae013b"},{"cell_type":"markdown","source":["To correct all blank and NULL values, we use the following:"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"9bb8c7f6-284c-46e3-8299-4ff8b0435000"},{"cell_type":"code","source":["import numpy as np\n","\n","# Replace placeholder values with NaN\n","placeholders = ['N/A', '', 'None', 'none', 'null', 'NULL']\n","df_pandas.replace(placeholders, np.nan, inplace=True)"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"89d70923-9438-4686-9909-3c3e70d4fc35"},{"cell_type":"code","source":["df_pandas[df_pandas['address'].isna()]"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"d647302d-9371-445b-ba8e-6b5c080df501"},{"cell_type":"markdown","source":["Where the date of birth is in the future, we simply remove these cases using the below:"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"cd694f4f-dfc8-4526-9367-14eec11db150"},{"cell_type":"code","source":["# Remove unrealistic dates of birth (e.g., future dates)\n","today = pd.Timestamp.today()\n","df_pandas = df_pandas[df_pandas['date_of_birth'] <= today].copy()"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"1cdeba2f-df99-4557-b8fe-fa881b34b035"},{"cell_type":"code","source":["df_pandas[df_pandas['date_of_birth'] >= today]"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"3e9eac11-f1e1-44c1-b627-901839b233c4"},{"cell_type":"markdown","source":["Finally, for cosmetic purposes, we will:\n","* Extract the postcode from the address field, remove from the address field and remove spaces in the postcode\n","* Remove special characters from the address\n","* Uppercase all string fields"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"bd318d29-8ad1-4468-9b0f-b88b1ad38560"},{"cell_type":"code","source":["# Extract postcode\n","postcode_pattern = r'(\\b[A-Z]{1,2}[0-9][0-9A-Z]?\\s?[0-9][A-Z]{2}\\b)'\n","\n","df_pandas['postcode'] = df_pandas['address'].str.extract(postcode_pattern, flags=re.IGNORECASE)\n","\n","df_pandas['address'] = df_pandas['address'].str.replace(postcode_pattern, '', regex = True)"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"6be27b35-df9f-4405-b842-4e226107da1d"},{"cell_type":"code","source":["df_pandas['postcode'] = df_pandas['postcode'].str.replace(' ', '')"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"02a91a34-57d7-415a-ba73-ed1b0ff286b8"},{"cell_type":"code","source":["# Apply uppercase and remove special characters from all string columns\n","for col in df_pandas.select_dtypes(include='object').columns:\n","    df_pandas[col] = df_pandas[col].apply(\n","        lambda x: re.sub(r'[^A-Z0-9 ]', '', x.upper()) if isinstance(x, str) else x\n","    )"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"77037af2-5504-47cb-9cb9-7bc50dee71cd"},{"cell_type":"code","source":["df_pandas"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"c32e7d79-d98f-4044-a6f0-471b8df195d2"},{"cell_type":"markdown","source":["### PySpark\n","\n","To be able to do the same operations in PySpark requires knowledge of it's syntax, which can be trickly to get to grips with. \n","\n","As discussed above, this initial learning curve allows us to run the data via Spark operations, which splits jobs between multiple computes. In effect, this is way quicker than using Pandas.\n","\n","The following code repeats the operations above using Pyspark."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"095d6f64-d91f-488f-806e-8a5e212fda62"},{"cell_type":"code","source":["from pyspark.sql.functions import (\n","    col, regexp_extract, regexp_replace, upper, trim, when,\n","    to_date, current_date, lit\n",")\n","\n","# Start with the base DataFrame\n","df = spark.table(\"coffee_lakehouse.personal_df\")\n","\n","# Extract title from name\n","df = df.withColumn(\n","    \"title\",\n","    regexp_extract(col(\"name\"), r\"^(Mrs|Mr|Ms|Dr|Prof|Rev|Sir|Madam)\", 1)\n",")\n","\n","# Remove title from name\n","df = df.withColumn(\n","    \"name\",\n","    regexp_replace(col(\"name\"), r\"^(Mrs|Mr|Ms|Dr|Prof|Rev|Sir|Madam)\", \"\")\n",")\n","\n","# Clean phone number: remove +44, brackets, and spaces\n","df = df.withColumn(\n","    \"phone_number\",\n","    regexp_replace(col(\"phone_number\"), r\"(\\+44\\(0\\)|\\+44|\\)|\\(| )\", \"\")\n",")\n","\n","# Replace placeholder values with nulls\n","for placeholder in ['N/A', '', 'None', 'none', 'null', 'NULL']:\n","    df = df.replace(placeholder, None)\n","\n","# Remove future dates of birth\n","df = df.filter(col(\"date_of_birth\") <= current_date())\n","\n","# Extract postcode from address\n","postcode_pattern = r\"(\\b[A-Z]{1,2}[0-9][0-9A-Z]?\\s?[0-9][A-Z]{2}\\b)\"\n","df = df.withColumn(\n","    \"postcode\",\n","    regexp_extract(col(\"address\"), postcode_pattern, 1)\n",")\n","\n","# Remove postcode from address\n","df = df.withColumn(\n","    \"address\",\n","    regexp_replace(col(\"address\"), postcode_pattern, \"\")\n",")\n","\n","# Remove spaces from postcode\n","df = df.withColumn(\n","    \"postcode\",\n","    regexp_replace(col(\"postcode\"), \" \", \"\")\n",")\n","\n","# Apply uppercase and remove special characters from all string columns\n","string_cols = [field.name for field in df.schema.fields if field.dataType.simpleString() == 'string']\n","for col_name in string_cols:\n","    df = df.withColumn(\n","        col_name,\n","        when(\n","            col(col_name).isNotNull(),\n","            regexp_replace(upper(col(col_name)), r\"[^A-Z0-9 ]\", \"\")\n","        ).otherwise(None)\n","    )\n","\n","display(df)"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"506bc0d2-34af-4357-aac4-27c91d23a56d"},{"cell_type":"markdown","source":["### Spark SQL\n","\n","Alternatively, we can use Spark SQL to perform these transformations. This way, we get to use SQL code, while retaining the power of Spark. \n","\n","To explore the data further, we can combine Pandas and Spark SQL logic here, using Python loops to run multiple SQL queries at once to explore all column counts."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"a40e7c1a-e00d-4641-bb81-f85c8f24a994"},{"cell_type":"code","source":["# Read the data from the lakehouse\n","df_spark = spark.sql(\"\"\"\n","SELECT\n","    patient_id,\n","    NULLIF(UPPER(REGEXP_EXTRACT(name, '^(Mrs|Mr|Ms|Dr|Prof|Rev|Sir|Madam)', 1)), '') AS title,\n","    UPPER(REGEXP_REPLACE(name, '^(Mrs|Mr|Ms|Dr|Prof|Rev|Sir|Madam)', '')) AS name,\n","    TO_DATE(date_of_birth) AS date_of_birth,\n","    NULLIF(UPPER(TRIM(CONCAT_WS(',', SLICE(SPLIT(address, ','), 1, SIZE(SPLIT(address, ',')) - 1)))), '') AS address,\n","    NULLIF(TRIM(REGEXP_REPLACE(ELEMENT_AT(SPLIT(address, ','), -1), ' ', '')), '') AS postcode,\n","    NULLIF(REGEXP_REPLACE(phone_number, '(\\\\\\\\+44\\\\\\\\(0\\\\\\\\)|\\\\\\\\+44|\\\\\\\\)|\\\\\\\\()| ', ''), 'N/A') AS phone_number,\n","    is_public_patient\n","FROM \n","    coffee_lakehouse.personal_df\n","WHERE\n","    date_of_birth <= current_date()\n","\"\"\")\n","display(df)"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"c845156c-cea2-44eb-a1b8-e13a3d412aa9"},{"cell_type":"markdown","source":["## Appointments Data Exploration\n","\n","Turning our attention to the appointments data, we only need to clean the `doctor_seen` column as discussed above."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"ff9a6f8b-29f9-4570-8c44-0f0732a15819"},{"cell_type":"code","source":["app_df = spark.sql(\"SELECT * FROM coffee_lakehouse.appointment_df\")\n","display(app_df)"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"dd2cddd7-0791-44d4-9b13-733b2490d332"},{"cell_type":"markdown","source":["Again, we can do this via Spark SQL like this:"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"3e77b741-d949-40cd-bbcc-3544083f5cc2"},{"cell_type":"code","source":["# Read the data from the lakehouse\n","df_app = spark.sql(\"\"\"\n","SELECT\n","    patient_id,\n","    appointment_date,\n","    NULLIF(UPPER(REGEXP_EXTRACT(doctor_seen, '^(Mrs|Mr|Ms|Dr|Prof|Rev|Sir|Madam)', 1)), '') AS doctor_title,\n","    UPPER(REGEXP_REPLACE(doctor_seen, '^(Mrs|Mr|Ms|Dr|Prof|Rev|Sir|Madam)', '')) AS doctor_seen\n","FROM \n","    coffee_lakehouse.appointment_df\n","\"\"\")\n","display(df_app)"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"9d33c51e-9579-4148-a5ad-258d4cabfb5e"},{"cell_type":"markdown","source":["## Write to Lakehouse"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"9f918f0f-ef46-4c80-ae50-afb6b7a824e6"},{"cell_type":"markdown","source":["Finally, we can write our results back to the lakehouse to save them. \n","\n","**First we must paste in our abfs path that we used before.**"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"b140434b-f221-4858-b813-b8efeac1adcd"},{"cell_type":"code","source":["# Specify lakehouse path\n","abfs_path = 'abfss://490a35a8-ffa1-4c26-8ad2-f394ba2aaefd@onelake.dfs.fabric.microsoft.com/ec5cde9a-5530-4099-ae29-d318b1970f64/Tables'"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"69b12da4-ceef-44e0-aff8-e78f6e1bd881"},{"cell_type":"markdown","source":["Then we use the following to upload the datasets:"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"c5dda94d-a7dc-4423-ad34-d6e1753778b8"},{"cell_type":"code","source":["(\n","df_spark\n","    .write\n","    .mode('overwrite')\n","    .format('delta')\n","    .option('overwriteSchema', 'true')\n","    .save(f\"{abfs_path}/personal_df_clean\")\n",")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"49c56564-128e-43e7-a540-81b43eea4162"},{"cell_type":"code","source":["(\n","df_app\n","    .write\n","    .mode('overwrite')\n","    .format('delta')\n","    .option('overwriteSchema', 'true')\n","    .save(f\"{abfs_path}/appointment_df_clean\")\n",")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"931d0540-891b-4058-b352-735ba535846a"}],"metadata":{"language_info":{"name":"python"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"microsoft":{"language":"python","ms_spell_check":{"ms_spell_check_language":"en"},"language_group":"synapse_pyspark"},"widgets":{},"kernel_info":{"name":"synapse_pyspark"},"nteract":{"version":"nteract-front-end@1.0.0"},"notebook_environment":{},"synapse_widget":{"version":"0.1","state":{}},"save_output":true,"spark_compute":{"compute_id":"/trident/default","session_options":{"enableDebugMode":false,"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"known_lakehouses":[{"id":"6d7d32eb-1533-480f-8d39-a181139f1665"},{"id":"ec5cde9a-5530-4099-ae29-d318b1970f64"}],"default_lakehouse":"ec5cde9a-5530-4099-ae29-d318b1970f64","default_lakehouse_name":"coffee_lakehouse","default_lakehouse_workspace_id":"490a35a8-ffa1-4c26-8ad2-f394ba2aaefd"}}},"nbformat":4,"nbformat_minor":5}