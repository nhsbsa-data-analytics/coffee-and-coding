{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment Analysis of IMDB Movie Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment analysis is a natural language processing (NLP) technique used to determine the emotional tone behind a body of text. It helps in identifying and categorizing opinions expressed in a piece of content as positive, negative, or neutral. Sentiment analysis is widely used in various domains, including:\n",
    "\n",
    "Customer feedback analysis: Understanding opinions in reviews, social media, or surveys.\n",
    "Market research: Analyzing sentiment about products, brands, or competitors.\n",
    "Public opinion: Assessing sentiment in political campaigns, news articles, or social media trends.\n",
    "\n",
    "Applications:\n",
    "Social media monitoring: Companies analyze tweets, comments, and posts to gauge public sentiment about their products or services.\n",
    "Customer service: Automated systems detect customer dissatisfaction and prioritize responses.\n",
    "Financial market predictions: Sentiment from news and social media is used to forecast market trends.\n",
    "Sentiment analysis helps organizations better understand customer opinions and improve decision-making by identifying trends and insights hidden in textual data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In this session, we will be processing the imdb movie reviews to develop a machine learning model to predict sentiment of reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/nhsbsa-data-analytics/coffee-and-coding/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from wordcloud import WordCloud,STOPWORDS\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK (Natural Language Toolkit) is a popular open-source library in Python for working with human language data (text). It provides tools for a wide variety of Natural Language Processing (NLP) tasks such as tokenization, parsing, stemming, tagging, and sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "# Download stopwords corpus\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"/content/coffee-and-coding/2024-09-25 Sentiment Analysis/IMDB Dataset.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataFrame Summary and Missing Values Analysis\n",
    "This code defines two functions: missing_values_analysis(df) and check_df(df). It helps analyze a given DataFrame (df) by:\n",
    "\n",
    "Checking the shape of the DataFrame (number of rows and columns).\n",
    "Identifying the data types of each column.\n",
    "Performing a missing values analysis, showing the total missing values and their percentage in each column that contains missing data.\n",
    "Displaying the first few rows of the DataFrame for a quick preview."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the number of rows and columns\n",
    "def missing_values_analysis(df):\n",
    "    na_columns_ = [col for col in df.columns if df[col].isnull().sum() > 0]\n",
    "    n_miss = df[na_columns_].isnull().sum().sort_values(ascending=True)\n",
    "    ratio_ = (df[na_columns_].isnull().sum() / df.shape[0] * 100).sort_values(ascending=True)\n",
    "    missing_df = pd.concat([n_miss, np.round(ratio_, 2)], axis=1, keys=['Total Missing Values', 'Ratio'])\n",
    "    missing_df = pd.DataFrame(missing_df)\n",
    "    return missing_df\n",
    "\n",
    "def check_df(df, head=5):\n",
    "    print(\"--------------------- Shape --------------------\")\n",
    "    print(df.shape)\n",
    "    print(\"-------------------- Types ---------------------\")\n",
    "    print(df.dtypes)\n",
    "    print(\"----------------- NaN Analysis -----------------\")\n",
    "    print(missing_values_analysis(df))\n",
    "    print(\"--------------------- Head ---------------------\")\n",
    "    print(df.head())\n",
    "\n",
    "check_df(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment Distribution Visualization\n",
    "Pie Chart:\n",
    "\n",
    "Visualizes the percentage breakdown of positive and negative sentiments.\n",
    "The pie chart uses two colors and includes an exploded slice for better emphasis.\n",
    "\n",
    "Count Plot:\n",
    "\n",
    "Displays a bar chart showing the count of positive and negative sentiments.\n",
    "Uses the same color palette as the pie chart for consistency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a , ax = plt.subplots(1,2,figsize=(22,9))\n",
    "data['sentiment'].value_counts().plot.pie(explode=[0,0.1], autopct='%1.2f%%',ax=ax[0],shadow=True, startangle=300, colors = [\"#bcbddc\", \"#efedf5\"])\n",
    "ax[0].set_title('Distribution of Positive / Negative Emotions')\n",
    "ax[0].set_ylabel('')\n",
    "sns.countplot(x='sentiment', data=data, ax=ax[1], palette=[\"#bcbddc\", \"#efedf5\"])\n",
    "ax[1].set_title('Distribution of Positive / Negative Emotions')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Its a balanced dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word cloud -Most common Negative words used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list=data.loc[data['sentiment']=='negative','review'].tolist()\n",
    "text=\"\"\n",
    "for sent in text_list:\n",
    "    sent=re.sub(\"[<br>]\",\" \",sent)\n",
    "    text=re.sub(\"[^A-Za-z0-9]+\",\" \",sent)\n",
    "    text+=sent\n",
    "\n",
    "stopwords=set(STOPWORDS)\n",
    "wordcloud = WordCloud(width=1200,height=700,\n",
    "                background_color ='white',\n",
    "                stopwords = stopwords,\n",
    "                min_font_size = 10).generate(text)\n",
    "\n",
    "# plot the WordCloud image\n",
    "plt.figure(figsize = (12, 7), facecolor = None)\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad = 0)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list=data.loc[data['sentiment']=='positive','review'].tolist()\n",
    "text=\"\"\n",
    "for sent in text_list:\n",
    "    sent=re.sub(\"[<br>]\",\" \",sent)\n",
    "    text=re.sub(\"[^A-Za-z0-9]+\",\" \",sent)\n",
    "    text+=sent\n",
    "\n",
    "stopwords=set(STOPWORDS)\n",
    "wordcloud = WordCloud(width=700,height=700,\n",
    "                background_color ='white',\n",
    "                stopwords = stopwords,\n",
    "                min_font_size = 10).generate(text)\n",
    "\n",
    "# plot the WordCloud image\n",
    "plt.figure(figsize = (12, 7), facecolor = None)\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad = 0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_review = data.review[0]\n",
    "example_review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Natural Language Processing (NLP), text cleaning is a crucial step to prepare data for analysis or model training. \n",
    "1. Remove HTML Tags\n",
    "2. Lowercasing\n",
    "3. Remove Punctuation\n",
    "4. Remove Stop Words\n",
    "5. Tokenization\n",
    "6. Lemmatization/Stemming\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked. They are right, as this is exactly what happened with me.<br /><br />The first thing that struck me about Oz was its brutality and unflinching scenes of violence, which set in right from the word GO. Trust me, this is not a show for the faint hearted or timid. This show pulls no punches with regards to drugs, sex or violence. Its is hardcore, in the classic use of the word.<br /><br />It is called OZ as that is the nickname given to the Oswald Maximum Security State Penitentary. It focuses mainly on Emerald City, an experimental section of the prison where all the cells have glass fronts and face inwards, so privacy is not high on the agenda. Em City is home to many..Aryans, Muslims, gangstas, Latinos, Christians, Italians, Irish and more....so scuffles, death stares, dodgy dealings and shady agreements are never far away.<br /><br />I would say the main appeal of the show is due to the fact that it goes where other shows wouldn't dare. Forget pretty pictures painted for mainstream audiences, forget charm, forget romance...OZ doesn't mess around. The first episode I ever saw struck me as so nasty it was surreal, I couldn't say I was ready for it, but as I watched more, I developed a taste for Oz, and got accustomed to the high levels of graphic violence. Not just violence, but injustice (crooked guards who'll be sold out for a nickel, inmates who'll kill on order and get away with it, well mannered, middle class inmates being turned into prison bitches due to their lack of street skills or prison experience) Watching Oz, you may become comfortable with what is uncomfortable viewing....thats if you can get in touch with your darker side.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text Tokenization and Stopwords Removal\n",
    "\n",
    "This script performs tokenization and stopwords removal on text data within a DataFrame. The goal is to preprocess text data by breaking it into tokens and filtering out common stopwords.\n",
    "\n",
    "Components:\n",
    "1. Tokenization:\n",
    "    - Utilizes the `ToktokTokenizer` from the `nltk.tokenize` module to split text into tokens.\n",
    "\n",
    "2. Stopwords Removal:\n",
    "    - Uses a predefined list of English stopwords from `nltk.corpus.stopwords`.\n",
    "    - Removes stopwords from the text based on whether the text should be in lower case or not.\n",
    "\n",
    "Functions:\n",
    "- remove_stopwords(text, is_lower_case=False):\n",
    "    - Tokenizes the input text.\n",
    "    - Removes stopwords from the tokens.\n",
    "    - Returns the filtered text as a string.\n",
    "\n",
    "Parameters:\n",
    "- `text`: The input text to be processed.\n",
    "- `is_lower_case` (optional): Boolean flag to specify if the stopwords removal should consider lowercased tokens (default: False).\n",
    "\n",
    "Usage:\n",
    "1. Import necessary libraries and initialize the tokenizer and stopword list.\n",
    "2. Define the `remove_stopwords` function to process the text data.\n",
    "3. Create a DataFrame with a 'review' column containing text data.\n",
    "4. Apply the `remove_stopwords` function to the 'review' column to preprocess the text.\n",
    "\n",
    "Example:\n",
    "    # Example DataFrame\n",
    "    data = pd.DataFrame({\n",
    "        'review': [\n",
    "            \"This is a sample review with some common words.\",\n",
    "            \"Another review with more text and stopwords.\",\n",
    "            \"A clean review after removing stopwords.\"\n",
    "        ]\n",
    "    })\n",
    "\n",
    "    # Apply the remove_stopwords function to the review column\n",
    "    data['review'] = data['review'].apply(remove_stopwords)\n",
    "\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenization of text\n",
    "tokenizer=ToktokTokenizer()\n",
    "#Setting English stopwords\n",
    "stopword_list=nltk.corpus.stopwords.words('english')\n",
    "\n",
    "\n",
    "#removing the stopwords\n",
    "def remove_stopwords(text, is_lower_case=False):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    if is_lower_case:\n",
    "        filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
    "    else:\n",
    "        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n",
    "    filtered_text = ' '.join(filtered_tokens)\n",
    "    return filtered_text\n",
    "#Apply function on review column\n",
    "data['review']=data['review'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Text Cleaning for Reviews\n",
    "\n",
    "This script  specifically targets HTML tags and text enclosed within square brackets for removal.\n",
    "\n",
    "Functions:\n",
    "- strip_html(text): Removes HTML tags from the input text using BeautifulSoup.\n",
    "- remove_between_square_brackets(text): Removes any text enclosed in square brackets, including the brackets themselves.\n",
    "- de_noise_text(text): Applies the above cleaning functions to remove HTML tags and text within square brackets.\n",
    "\n",
    "Usage:\n",
    "1. Define the functions for text cleaning.\n",
    "2. Create a DataFrame with a 'review' column containing text data.\n",
    "3. Apply the `de_noise_text` function to the 'review' column to clean the text.\n",
    "\n",
    "Example:\n",
    "    # Example DataFrame\n",
    "    data = pd.DataFrame({\n",
    "        'review': [\n",
    "            \"This is a sample review <b>with HTML</b> and [some unnecessary text].\",\n",
    "            \"Another review without HTML but [with brackets] around.\",\n",
    "            \"Just a clean review without any noise.\"\n",
    "        ]\n",
    "    })\n",
    "    \n",
    "    # Apply the de_noise_text function to the review column\n",
    "    data['review'] = data['review'].apply(de_noise_text)\n",
    "    \n",
    "    print(data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing the html strips\n",
    "def strip_html(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "#Removing the square brackets\n",
    "def remove_between_square_brackets(text):\n",
    "    return re.sub('\\[[^]]*\\]', '', text)\n",
    "\n",
    "#Removing the noisy text\n",
    "def de_noise_text(text):\n",
    "    text = strip_html(text)\n",
    "    text = remove_between_square_brackets(text)\n",
    "    return text\n",
    "#Apply function on review column\n",
    "data['review']=data['review'].apply(de_noise_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Special Characters Removal from Text\n",
    "\n",
    "This script defines a function to remove special characters from text data. The function is used to preprocess text by eliminating unwanted characters, leaving only alphanumeric characters and spaces.\n",
    "\n",
    "Components:\n",
    "1. Function Definition:\n",
    "    - `remove_special_characters(text, remove_digits=True)`: A function to remove special characters from the input text.\n",
    "\n",
    "Parameters:\n",
    "- `text` (str): The input text from which special characters will be removed.\n",
    "- `remove_digits` (bool, optional): A flag to indicate if digits should be removed (default: True). This parameter is currently not used in the function but can be incorporated for more control if needed.\n",
    "\n",
    "Returns:\n",
    "- str: The cleaned text with special characters removed.\n",
    "\n",
    "Usage:\n",
    "1. Define the `remove_special_characters` function to process the text data.\n",
    "2. Create a DataFrame with a 'review' column containing text data.\n",
    "3. Apply the `remove_special_characters` function to the 'review' column to preprocess the text by removing special characters.\n",
    "\n",
    "Example:\n",
    "    # Example DataFrame\n",
    "    data = pd.DataFrame({\n",
    "        'review': [\n",
    "            \"This is a sample review with special characters! @#$%\",\n",
    "            \"Another review with some digits 1234 and symbols.\",\n",
    "            \"Clean text after removing special characters.\"\n",
    "        ]\n",
    "    })\n",
    "\n",
    "    # Apply the remove_special_characters function to the review column\n",
    "    data['review'] = data['review'].apply(remove_special_characters)\n",
    "\n",
    "    print(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define function for removing special characters\n",
    "def remove_special_characters(text, remove_digits=True):\n",
    "    pattern=r'[^a-zA-z0-9\\s]'\n",
    "    text=re.sub(pattern,'',text)\n",
    "    return text\n",
    "#Apply function on review column\n",
    "data['review']=data['review'].apply(remove_special_characters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming Text Data\n",
    "\n",
    "This script defines a function to apply stemming to text data. Stemming is a process in natural language processing (NLP) that reduces words to their root form. This function uses the Porter Stemmer from the NLTK library to perform stemming on each word in the text.\n",
    "\n",
    "Components:\n",
    "1. Function Definition:\n",
    "    - `simple_stemmer(text)`: A function to stem words in the input text using the Porter Stemmer.\n",
    "\n",
    "Parameters:\n",
    "- `text` (str): The input text on which stemming will be applied.\n",
    "\n",
    "Returns:\n",
    "- str: The text with words reduced to their root forms.\n",
    "\n",
    "Usage:\n",
    "1. Define the `simple_stemmer` function to process the text data.\n",
    "2. Create a DataFrame with a 'review' column containing text data.\n",
    "3. Apply the `simple_stemmer` function to the 'review' column to preprocess the text by stemming.\n",
    "\n",
    "Example:\n",
    "    # Example DataFrame\n",
    "    data = pd.DataFrame({\n",
    "        'review': [\n",
    "            \"Running quickly through the fields.\",\n",
    "            \"The stems of the plants are green.\",\n",
    "            \"Stemming words helps in text analysis.\"\n",
    "        ]\n",
    "    })\n",
    "\n",
    "    # Apply the simple_stemmer function to the review column\n",
    "    data['review'] = data['review'].apply(simple_stemmer)\n",
    "\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stemming the text\n",
    "def simple_stemmer(text):\n",
    "    ps=nltk.porter.PorterStemmer()\n",
    "    text= ' '.join([ps.stem(word) for word in text.split()])\n",
    "    return text\n",
    "#Apply function on review column\n",
    "data['review']=data['review'].apply(simple_stemmer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract and Preview Normalized Training and Testing Reviews\n",
    "\n",
    "This script segment is responsible for splitting the normalized review data into training and testing datasets. The reviews are extracted based on their indices, where the first 40,000 reviews are designated for training and the remaining reviews are reserved for testing.\n",
    "\n",
    "Components:\n",
    "1. Training Reviews:\n",
    "   - Extract the first 40,000 reviews from the normalized data for training purposes.\n",
    "   - Preview a sample review from this subset.\n",
    "\n",
    "2. Testing Reviews:\n",
    "   - Extract reviews from index 40,000 onward for testing purposes.\n",
    "   - Preview a sample review from this subset.\n",
    "\n",
    "Parameters:\n",
    "- `data.review` (pd.Series): A Pandas Series containing the normalized review texts.\n",
    "\n",
    "Returns:\n",
    "- `norm_train_reviews` (pd.Series): A Series containing the training reviews.\n",
    "- `norm_test_reviews` (pd.Series): A Series containing the testing reviews.\n",
    "\n",
    "Usage:\n",
    "1. Extract the required reviews for model training and evaluation.\n",
    "2. Use these subsets to train models and assess their performance on unseen test data.\n",
    "\n",
    "Example:\n",
    "    # Extract and preview normalized reviews for training and testing\n",
    "    norm_train_reviews = data.review[:40000]\n",
    "    print(norm_train_reviews[0])  # Preview a training review\n",
    "    \n",
    "    norm_test_reviews = data.review[40000:]\n",
    "    print(norm_test_reviews[40001])  # Preview a testing review\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Binarization of Sentiment Data\n",
    "# Initialize the LabelBinarizer to convert sentiment labels into binary format\n",
    "lb = LabelBinarizer()\n",
    "\n",
    "# Transform the 'sentiment' column into a binary matrix\n",
    "# Each unique sentiment label will be converted into a binary vector\n",
    "sentiment_data = lb.fit_transform(data['sentiment'])\n",
    "\n",
    "# Print the shape of the resulting binary matrix\n",
    "# This will show the number of samples (rows) and the number of binary features (columns)\n",
    "print(sentiment_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting Sentiment Data into Training and Testing Sets\n",
    "# Separate the sentiment data into training and testing sets\n",
    "# The training set consists of the first 40,000 samples\n",
    "# The testing set consists of the remaining samples\n",
    "\n",
    "# Extracting the first 40,000 samples for training\n",
    "#normalized train reviews\n",
    "norm_train_reviews=data.review[:40000]\n",
    "norm_train_reviews[0]\n",
    "# Extracting the remaining samples for testing\n",
    "#Normalized test reviews\n",
    "norm_test_reviews=data.review[40000:]\n",
    "norm_test_reviews[40001]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting Sentiment Data into Training and Testing Sets\n",
    "\n",
    "# The training set consists of the first 40,000 samples\n",
    "# The testing set consists of the remaining samples\n",
    "\n",
    "# Extracting the first 40,000 samples for training\n",
    "train_sentiment_y = sentiment_data[:40000]\n",
    "\n",
    "# Extracting the remaining samples for testing\n",
    "test_sentiment_y = sentiment_data[40000:]\n",
    "\n",
    "# Print the training and testing sentiment data\n",
    "# This will display the content of the training and testing sets to verify the split\n",
    "print(train_sentiment_y)\n",
    "print(test_sentiment_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Bag of Words (BoW) Model using Count Vectorizer\n",
    "# This code snippet demonstrates how to use CountVectorizer to convert text data into a matrix of token counts.\n",
    "# It handles tokenization into unigrams, bigrams, and trigrams, and processes both training and test datasets.\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Initialize CountVectorizer with specified parameters\n",
    "vectorizer = CountVectorizer(max_features=10000)\n",
    "\n",
    "# Transform the training reviews into a matrix of token counts\n",
    "train_x = vectorizer.fit_transform(norm_train_reviews)\n",
    "\n",
    "# Transform the test reviews into a matrix of token counts using the same vocabulary\n",
    "test_x = vectorizer.transform(norm_test_reviews)\n",
    "# Output the shape of the transformed matrices\n",
    "print('BOW_cv_train:', train_x.shape)\n",
    "print('BOW_cv_test:', test_x.shape)\n",
    "\n",
    "# Retrieve feature names (terms/ngrams) used for tokenization\n",
    "# vocab = cv.get_feature_names() # Uncomment to get feature names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "\n",
    "models.append(('Logistic Regression',LogisticRegression()))\n",
    "models.append(('Random Forest Classifier',RandomForestClassifier()))\n",
    "models.append(('Decision Tree Classifier',DecisionTreeClassifier()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, model in models:\n",
    "\n",
    "    %time model.fit(train_x, train_sentiment_y)\n",
    "    test_pred = model.predict(test_x)\n",
    "    #print(name ,'Accuracy Score : ',accuracy_score(test_sentiment_y, test_pred))\n",
    "    #print(name ,'F1 Score : ',f1_score(test_sentiment_y, test_pred, average='weighted'))\n",
    "    #confusion matrix for bag of words\n",
    "    cm=confusion_matrix(test_sentiment_y,test_pred,labels=[1,0])\n",
    "    # Extracting TP, TN, FP, FN\n",
    "    tn, fp, fn, tp = cm.ravel()  # .ravel() flattens the confusion matrix into a 1D array\n",
    "\n",
    "    # Print values of TP, TN, FP, FN\n",
    "    print(f'True Positives (TP): {tp}')\n",
    "    print(f'True Negatives (TN): {tn}')\n",
    "    print(f'False Positives (FP): {fp}')\n",
    "    print(f'False Negatives (FN): {fn}')\n",
    "    # Display confusion matrix using seaborn heatmap for better visualization\n",
    "    plt.figure(figsize=(6,4))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Predicted 0', 'Predicted 1'], yticklabels=['Actual 0', 'Actual 1'])\n",
    "    plt.ylabel('Actual')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "    report = classification_report(test_sentiment_y, test_pred, target_names=['Class 0', 'Class 1'])\n",
    "\n",
    "    # Print the classification report\n",
    "    print(report)\n",
    "    print('-----------------------------------------------------------------------')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
