{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPXsgCQ3c2CUjtSOv3LW6rL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nhsbsa-data-analytics/coffee-and-coding/blob/20250130-aj-what-is-spark/2025_01_30_coffee_and_coding_intro_to_spark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Coffee & Coding: What is Spark?\n",
        "\n",
        "- **Date:** 30/01/2025\n",
        "- **Presented by:** Alistair Jones\n",
        "\n",
        "## Overview\n",
        "This session provides an introduction to Spark by example.\n",
        "We will explore basic operations to read, transform and write data using Spark SQL and PySpark.\n",
        "We will then work through a short practical example of a simple analytical pipeline.\n",
        "\n",
        "The session will cover the fundamentals of how Spark works, why it is important/useful and offer some considerations for using it in your workflows.\n",
        "\n",
        "## Background and context\n",
        "\n",
        "### Why should you care about Spark?\n",
        "Apache Spark is a unified analytics engine designed for large-scale data processing.\n",
        "Some of the strengths of Spark include:\n",
        "\n",
        "- **Scalability** is the primary benefit of using Spark. It distributes work across a cluster of computers, which means it can handle much bigger workloads than a single computer could by itself.\n",
        "- **Support for multiple languages** auch as Python via PySpark and SQL.\n",
        "- **Managed compute** in platforms such as Microsoft Fabric  \n",
        "- Spark is completely **open-source** and **well-documented**!\n",
        "- Spark has an **active community** of developers with lots of helpful threads and tutorials available online.\n",
        "\n",
        "### What are the limitations of Spark?\n",
        "- **Additional complexity** due to the distributed nature of Spark, which means you need a cluster of computers to leverage it\n",
        "- **Row-wise computation** means we need to be careful about things like row-ordering (we will show this below)\n",
        "- Can be **hard to debug** due to the distributed nature and because it is implemented in Java under the hood (which means lengthy errors and stacktraces!)\n",
        "- Can be **hard to setup** due to the need for a cluster and the vast configuration options!"
      ],
      "metadata": {
        "id": "851a1xcplfi1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Getting started\n"
      ],
      "metadata": {
        "id": "QrRqBJZI26S2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Environment Setup\n",
        "The first step is to setup our environment with the packages and configuration required to do the work.\n",
        "\n",
        "#### Installing PySpark Locally\n",
        "Note that Apache Spark (PySpark and Spark SQL) are meant to run on a distributed system.\n",
        "By distributed, we just mean a cluster of computers (or nodes) all working together to perform a task.\n",
        "\n",
        "You can install PySpark locally to learn how to write and run code via the PySpark API.\n",
        "Since this is on a single computer rather than a distributed system, while it will look the same, the way it works under the hood will be different in the different environments.\n",
        "\n",
        "This has implications for performance and certain behaviours.\n",
        "It won't be a problem for learning how to interact with the PySpark API, but it is a consideration for transfering this over to an actual Spark system (such as Microsoft Fabric)."
      ],
      "metadata": {
        "id": "6PMm1SI8HcF1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C3JkH_5t3EVB",
        "outputId": "b575b422-5cb5-4991-ec37-b0d021b2a0f3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.11/dist-packages (3.5.4)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Packages"
      ],
      "metadata": {
        "id": "b-pLbjbB5gQ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkFiles\n",
        "from pyspark.sql import SparkSession, functions as F"
      ],
      "metadata": {
        "id": "jn2OKNxB28tx"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### What is a Spark Session?\n",
        "A Spark Session is the entry point to programming Spark. It provides a way to interact with Spark's functionality and create DataFrames.\n",
        "\n",
        "In managed data platforms such as Microsoft Fabric, the Spark Session is usually created when you start the notebook and available via a global `spark` variable without any further setup.\n",
        "\n",
        "In this example, since we are in our own environment, we need to create a Spark Session to be able to work with Spark."
      ],
      "metadata": {
        "id": "zuo9KF5RlUx9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Spark Session since we are not working in an environment\n",
        "# where one is created for us\n",
        "spark = (\n",
        "    SparkSession.builder\n",
        "    .appName(\"Coffee_And_Coding\")\n",
        "    .getOrCreate()\n",
        ")"
      ],
      "metadata": {
        "id": "YCjqwqgR2LHJ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading Inputs from the NHSBSA Open Data Portal\n",
        "Usually when we work in a managed environment like a data platform, we have access to data stored in tables that already exist and we just want to do something with that data.\n",
        "For this tutorial there is an additional setup step so that we can interact with\n",
        "data like we would in a data platform: we will download a csv file from the NHSBSA Open Data Portal and load it into a table, so that we have something to query!"
      ],
      "metadata": {
        "id": "zRuFR3xdEJsT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First we download the csv file and load it into a PySpark DataFrame."
      ],
      "metadata": {
        "id": "sOqvVKvSFwbK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding the file to the Spark Context creates a reference to the data\n",
        "# so that we can query it,\n",
        "data_url = \"https://opendata.nhsbsa.net/dataset/a436f2d3-e6c8-46bd-b730-4f4d7af7ca56/resource/9e2d01a5-7644-4843-809a-b09ddd6f447a/download/monthly-hospital-data-oct24.csv\"\n",
        "spark.sparkContext.addFile(data_url)\n",
        "\n",
        "# Get the local file spark created\n",
        "filename = data_url.split(\"/\")[-1]  # Part of the URL after the last '/'\n",
        "local_filepath = f\"file://{SparkFiles.get(filename)}\"\n",
        "\n",
        "# Load the data from a csv\n",
        "df = (\n",
        "    spark.read\n",
        "    .option(\"header\", \"true\")\n",
        "    .option(\"inferSchema\", \"true\")\n",
        "    .csv(local_filepath)\n",
        ")"
      ],
      "metadata": {
        "id": "VG34K1ywBuWg"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we can write the data to a table.\n",
        "This step creates [parquet](https://parquet.apache.org/) files in our local drive which are the storage format Spark uses under the hood.\n",
        "Spark works by distributing a dataset around the cluster by sending a subset of the parquet files to each node.\n",
        "\n",
        "This is how Spark systems are able to scale so well, because each node in the cluster only has to do a small portion of the work.\n",
        "If we add more nodes, or make the nodes bigger, then we can do more work in a way that wouldn't be possible with a single computer!"
      ],
      "metadata": {
        "id": "nc3nz6wEF1qR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Store the data in a new table\n",
        "table_name = filename.split(\".\")[0]  # Part of the filename before .csv\n",
        "table_name = table_name.replace(\"-\", \"_\")  # Sanitize the table name\n",
        "\n",
        "# There are many options to configure the table to be written\n",
        "# We are only using two which provide the name and indicate\n",
        "# that we should overwrite the table if it already exists\n",
        "df.write.mode(\"overwrite\").saveAsTable(table_name)\n",
        "\n",
        "# Pick up from here"
      ],
      "metadata": {
        "id": "fNPYfMV2FtLf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "outputId": "dc9e5982-a68f-429f-89e0-ce61afa7dcbd"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SparkRuntimeException",
          "evalue": "[LOCATION_ALREADY_EXISTS] Cannot name the managed table as `spark_catalog`.`default`.`monthly_hospital_data_oct24`, as its associated location 'file:/content/spark-warehouse/monthly_hospital_data_oct24' already exists. Please pick a different table name, or remove the existing location first.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mSparkRuntimeException\u001b[0m                     Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-f3a9bca31077>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# We are only using two which provide the name and indicate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# that we should overwrite the table if it already exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"overwrite\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveAsTable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtable_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36msaveAsTable\u001b[0;34m(self, name, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m   1584\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1585\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1586\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveAsTable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1588\u001b[0m     def json(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mSparkRuntimeException\u001b[0m: [LOCATION_ALREADY_EXISTS] Cannot name the managed table as `spark_catalog`.`default`.`monthly_hospital_data_oct24`, as its associated location 'file:/content/spark-warehouse/monthly_hospital_data_oct24' already exists. Please pick a different table name, or remove the existing location first."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fundamental Operations\n",
        "In this section we'll step through a few of the fundamental operations for interacting with PySpark and Spark SQL.\n",
        "Many of these will be familiar to anyone who has used SQL before!\n",
        "\n",
        "In each part we'll show both the PySpark and the Spark SQL command: feel free to follow along to whichever you prefer.\n",
        "If you don't have any prior Python experience, the Spark SQL may be more accessible.\n",
        "Equally, PySpark can be a good entrypoint to learn some Python if you are already familiar with SQL!"
      ],
      "metadata": {
        "id": "IWvGGbDPlQbs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create a Dataframe From Input Data\n",
        "Spark can read data from many different sources: tables, csv files, web urls, etc.\n",
        "Since we are usually working in a managed environment where the tables are curated for us, we'll explore how to load data from a table.\n"
      ],
      "metadata": {
        "id": "Pz9WM0Kd6qff"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### What is a DataFrame?\n",
        "Whenever we read data using the PySpark API, we create a 'DataFrame', which represents the data we are querying.\n",
        "A DataFrame isn't actually the data itself: it is essentially a set of instructions to read, transform and write data.\n",
        "These instructions are evaluated 'lazily' which means that nothing happens straight away - you queue up all of your instructions and Spark works out when to actually do the work (usually this is when you want to print or write some results).\n",
        "\n",
        "Spark uses lazy evaluation to optimise query performance: by only executing the code at the point we need it to, Spark can cleverly work out the fastest and most efficient way to perform all the operations we are asking it to!"
      ],
      "metadata": {
        "id": "XTyUV-GyJFdz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### PySpark\n",
        "We can easily create a DataFrame in PySpark using the `spark.table` function."
      ],
      "metadata": {
        "id": "x7SemfEd6Z_S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.table(table_name)\n",
        "df.show(5)"
      ],
      "metadata": {
        "id": "vDS4IGdUHKeO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Spark SQL\n",
        "To query data with SQL, we use the `spark.sql` function, providing a query that look be familiar to current users of SQL."
      ],
      "metadata": {
        "id": "80CIuXb_6XvA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = f\"\"\"\n",
        "    SELECT * FROM {table_name} LIMIT 5\n",
        "\"\"\"\n",
        "print(f\"Showing result of: {query}\")\n",
        "spark.sql(query).show()"
      ],
      "metadata": {
        "id": "wR4d9OgF6Wrr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Selecting and Renaming Columns\n",
        "It is good practice to select only the columns we need to use in our analysis / pipeline / model.\n",
        "\n",
        "Column selection helps:\n",
        "- **Minimise data** to only the fields we need to use, thus decreasing the likelihood of any privacy risks!\n",
        "- **Improve query performance** by reducing the amount of data we pull through, reducing memory usage and processing time!\n",
        "- **Focus on relevant data** as some of our datasets can be very wide and it might not be helpful to see everything at once!"
      ],
      "metadata": {
        "id": "9cPK6xailMbk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### PySpark\n",
        "To select columns in PySpark, we use the `select` method on the DataFrame we created above."
      ],
      "metadata": {
        "id": "qb1E9Tks6hS-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Columns to select\n",
        "# We can put these directly in the arguments of 'select' but\n",
        "# it is often easier to read / maintain if we pull them out\n",
        "# into their own list\n",
        "columns = [\n",
        "    \"HOSPITAL TRUST CODE\",\n",
        "    \"BNF CODE\",\n",
        "    \"QUANTITY\",\n",
        "    \"PERIOD\",\n",
        "]\n",
        "selected_df = df.select(columns)\n",
        "selected_df.show(5)"
      ],
      "metadata": {
        "id": "pSSV5NSelMQR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice that our column names contain spaces: we may want to rename these to standardise and avoid potential problems down the line.\n",
        "\n",
        "In PySpark we can reference columns by name, or via a 'Column' object\n",
        "created with the `col` function from `pyspark.sql.functions` (aliased here as `F`). Using a Column object is often useful because we can perform operations on it, which enables us specify complex transformations in a modular way.\n",
        "Think about those times you've seen many lines to define a single column in the select part of a SQL query!"
      ],
      "metadata": {
        "id": "X4XpaXXOLa3u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "renamed_columns = []\n",
        "\n",
        "for old_name in df.columns:\n",
        "    old_col = F.col(old_name)  # Define the Column by referencing via its name\n",
        "\n",
        "    # Get the new name and use it to create a new Column by aliasing the old\n",
        "    new_name = old_name.replace(\" \", \"_\")  # Replace spaces with underscore\n",
        "    new_col = old_col.alias(new_name)\n",
        "\n",
        "    # Add the new column to the list of columns we will select\n",
        "    renamed_columns.append(new_col)\n",
        "\n",
        "# Select the newly renamed columns\n",
        "renamed_df = df.select(renamed_columns)\n",
        "print(\"Renamed columns:\")\n",
        "print(renamed_df.columns)"
      ],
      "metadata": {
        "id": "ryE6FRTBLNX-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Spark SQL\n",
        "In Spark SQL selecting columns is straightforward: we just modify the query we pass to `spark.sql`.\n",
        "The only issue to be aware of is that is can be hard to write modular code in this way, so while it might feel easier, sometimes it can be harder to write Spark SQL compared to PySpark due to readability concerns.\n",
        "\n",
        "Similar to above, we can rename our columns by aliasing."
      ],
      "metadata": {
        "id": "W3L5UZC86hly"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = f\"\"\"\n",
        "    SELECT\n",
        "        PERIOD AS PERIOD,\n",
        "        `BNF NAME` AS BNF_NAME,\n",
        "        `BNF CODE` AS BNF_CODE,\n",
        "        `HOSPITAL TRUST CODE` AS HOSPITAL_TRUST_CODE,\n",
        "        `HOSPITAL TRUST` AS HOSPITAL_TRUST,\n",
        "        QUANTITY AS QUANTITY,\n",
        "        `TOTAL QUANTITY` AS TOTAL_QUANTITY,\n",
        "        `TOTAL ITEMS` AS TOTAL_ITEMS,\n",
        "        `TOTAL ACTUAL COST` AS TOTAL_ACTUAL_COST,\n",
        "        `TOTAL NIC` AS TOTAL_NIC\n",
        "    FROM {table_name}\n",
        "    LIMIT 5\n",
        "\"\"\"\n",
        "spark.sql(query).show()"
      ],
      "metadata": {
        "id": "OmoyT6Ie6gqm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "renamed_df.filter(\"BNF_NAME like '%Paracetamol%'\").show(5)"
      ],
      "metadata": {
        "id": "dwBnAC8jM3tS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Filtering Rows\n",
        "Filtering rows is a key operation that we use frequently in preparing and transforming data for analysis, helping to:\n",
        "- **Minimise data** to only the records we need to use, thus decreasing the likelihood of any privacy risks!\n",
        "- **Improve query performance** by reducing the amount of data we pull through, reducing memory usage and processing time!\n",
        "- **Focus on relevant data** as some of our datasets can be very long and it might not be helpful to see everything at once!"
      ],
      "metadata": {
        "id": "xcJnb_X6lHSM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### PySpark\n",
        "In PySpark we can filter rows using the `filter` or `where` methods of a DataFrame (you can use either - they are exactly the same under the hood!).\n",
        "\n",
        "Filter conditions can be created either using a SQL expression withing a string (e.g. `\"my_column > 5\"`) or using the Column object we discussed above (e.g. `F.col(\"my_column\") > 5`. You can use either, but Column objects are usually preferable for more complex conditions, since they can be created and combined in a modular way (unlike string SQL expressions, Column objects support most operations like addition, subtraction, greater or less than, and so on)."
      ],
      "metadata": {
        "id": "xDESD7Z569C_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hospital_condition_col = F.col(\"HOSPITAL_TRUST_CODE\") == \"RKB00\"  # Coventry\n",
        "bnf_condition_col = F.substring(\"BNF_CODE\", 0, 9) == \"0407010H0\"  # Paracetamol\n",
        "condition_col = hospital_condition_col & bnf_condition_col\n",
        "filtered_df = renamed_df.filter(condition_col)\n",
        "filtered_df.show(5)"
      ],
      "metadata": {
        "id": "Yt2rnTlVlG-3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Spark SQL\n",
        "In Spark SQL, similarly to the sections above, we simply provide a SQL query with a 'WHERE' clause, as we would with other flavours of SQL.\n",
        "\n",
        "Note how the query size starts to grow: when we create a DataFrame with PySpark the query is dynamically created at runtime (i.e. after we've added all the transformations) but when we write SQL directly we have to put everything in one long query string.\n",
        "\n",
        "One way to get around this is to create temporary tables or views to hold intermediate outputs from SQL queries.\n",
        "We'll not cover that here though, instead simply recommending to give PySpark a try!"
      ],
      "metadata": {
        "id": "30ULAs1h67a6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO from here\n",
        "query = f\"\"\"\n",
        "    SELECT\n",
        "        PERIOD AS PERIOD,\n",
        "        `BNF NAME` AS BNF_NAME,\n",
        "        `BNF CODE` AS BNF_CODE,\n",
        "        `HOSPITAL TRUST CODE` AS HOSPITAL_TRUST_CODE,\n",
        "        `HOSPITAL TRUST` AS HOSPITAL_TRUST,\n",
        "        QUANTITY AS QUANTITY,\n",
        "        `TOTAL QUANTITY` AS TOTAL_QUANTITY,\n",
        "        `TOTAL ITEMS` AS TOTAL_ITEMS,\n",
        "        `TOTAL ACTUAL COST` AS TOTAL_ACTUAL_COST,\n",
        "        `TOTAL NIC` AS TOTAL_NIC\n",
        "    FROM {table_name}\n",
        "    WHERE `HOSPITAL TRUST CODE` = 'RKB00'\n",
        "        AND substring(`BNF CODE`, 0, 9) = '0407010H0'\n",
        "    LIMIT 5\n",
        "\"\"\"\n",
        "spark.sql(query).show()"
      ],
      "metadata": {
        "id": "CRKBjIwi666k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Adding New Columns\n",
        "\n",
        "Above we looked at how to select existing columns, but we can also add new columns to our data.\n",
        "This allows us to perform calculations involving existing columns or apply business rules and derivations."
      ],
      "metadata": {
        "id": "MrKgu213k5wL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### PySpark\n",
        "In Pyspark, we _can_ add new columnd with `select`, but it is often clearer if we stick to using `select` for picking existing columns and `withColumn` to add a new column.\n",
        "\n",
        "If the new column involves a conditional 'case-when' statement, we can do this using `when` from `pyspark.sql.functions`."
      ],
      "metadata": {
        "id": "ydBBvsDn9Hf5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract date from period\n",
        "derived_df = renamed_df.withColumn(\"DATE\", F.to_date(\"PERIOD\", \"yyyyMM\"))\n",
        "\n",
        "# Add a new column to classify the quantity\n",
        "# Case when statement using F.when(cond, value_if_true).otherwise(default)\n",
        "quantity_class_col = (\n",
        "    F.when(F.col(\"QUANTITY\") > 50, \"high\")\n",
        "    .otherwise(\"low\")\n",
        ")\n",
        "derived_df = derived_df.withColumn(\"QUANTIY_CLASS\", quantity_class_col)\n",
        "\n",
        "# Show the new columns and the columns used to derive them\n",
        "show_cols = [\n",
        "    \"PERIOD\",\n",
        "    \"DATE\",\n",
        "    \"QUANTITY\",\n",
        "    \"QUANTIY_CLASS\",\n",
        "]\n",
        "derived_df.select(show_cols).show(5)"
      ],
      "metadata": {
        "id": "KtuPht0vlAvW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: if we want to add a constant-value column, we need to use `lit` which means we are defining a literal value column.\n",
        "This is because PySpark expects columns to either be Column objects (which represent a SQL expression under the hood) or a string which references the column by name.  \n",
        "The `lit` function simply tells PySpark to create a Column with a single constant value."
      ],
      "metadata": {
        "id": "fnF7qvgpRBQN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Note that the line below doesn't reassign the DataFrame derived_df with the\n",
        "# new column (i.e. there is no '='), so this transformation will not be\n",
        "# preserved downstream\n",
        "(\n",
        "    derived_df\n",
        "    .withColumn(\"CONSTANT\", F.lit(1))  # Add the column\n",
        "    .select(\"CONSTANT\")  # Select the new column\n",
        "    .show(5)\n",
        ")"
      ],
      "metadata": {
        "id": "ORIniBJrRAMx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Spark SQL\n",
        "As with other SQL sections above, we can use familiar syntax to other flavours of SQL to add a new column via a `SELECT` statement and use `CASE... WHEN... ELSE...` to implement conditional column derivations."
      ],
      "metadata": {
        "id": "kDTUMMuh9JmA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# As above, we derive 2 columns\n",
        "# 'DATE' from formatting the 'PERIOD'\n",
        "# 'QUANTIY_CLASS' from conditional cases on 'QUANTIY'\n",
        "query = f\"\"\"\n",
        "    SELECT\n",
        "        PERIOD,\n",
        "        format_string(PERIOD, 'yyyyMM') AS DATE,\n",
        "        QUANTITY,\n",
        "        CASE\n",
        "            WHEN QUANTITY > 50 THEN 'high'\n",
        "            ELSE 'low'\n",
        "        END AS QUANTITY_CLASS\n",
        "    FROM {table_name}\n",
        "\"\"\"\n",
        "spark.sql(query).show(5)"
      ],
      "metadata": {
        "id": "bt5poLmB8ngT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Aggregations\n",
        "Preparing, transforming and analysing data often involves some form of aggregations.\n",
        "We can aggregate data through 'group by' operations, which will be familiar to many folks who have used languages such as SQL, Python or R to interact with data.\n",
        "\n",
        "There are two parts to a 'group by':\n",
        "1. The fields or columns in the data we want to group records by\n",
        "2. The fields or columns to be aggregated over the group and how they will be aggregated (e.g. summed, counted, etc)"
      ],
      "metadata": {
        "id": "dGhRjs-BlCVc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### PySpark\n",
        "In PySpark, we use the `groupBy` method of a DataFrame to perform a 'group by' operation and `agg` to do the aggregation.\n",
        "We can provide Column objects or strings to specify the fields to use in each step of this transformation and as above we often want to use Column objects.\n",
        "\n",
        "Additionally, it is often easier to read, understand and maintain code where the columns have been pulled out of the transformation itself, as shown below."
      ],
      "metadata": {
        "id": "cV02f8_59MVy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lift the columns for grouping and aggregation out of the transformation\n",
        "# This can make code easier to read and maintain, especially as the complexity\n",
        "# increases\n",
        "group_cols = [\n",
        "    F.col(\"HOSPITAL_TRUST_CODE\"),\n",
        "    F.col(\"BNF_CODE\"),\n",
        "]\n",
        "agg_cols = [\n",
        "    # `alias` used to name the resulting column\n",
        "    F.sum(\"TOTAL_QUANTITY\").alias(\"TOTAL_QUANTITY\")\n",
        "]\n",
        "totals_by_hospital_and_bnf_df = (\n",
        "    renamed_df\n",
        "    .groupBy(*group_cols)\n",
        "    .agg(*agg_cols)\n",
        ")\n",
        "\n",
        "totals_by_hospital_and_bnf_df.show(5)"
      ],
      "metadata": {
        "id": "CVVB3tRslBiw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Spark SQL\n",
        "Aggregation in Spark SQL uses the familiar 'GROUP BY' expression from other flavours of SQL, along with aggregation expressions such as 'sum' in the columns to be selected."
      ],
      "metadata": {
        "id": "17760m659NuC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = f\"\"\"\n",
        "    SELECT\n",
        "        `HOSPITAL TRUST CODE` AS HOSPITAL_TRUST_CODE,\n",
        "        `BNF CODE` AS BNF_CODE,\n",
        "        sum(`TOTAL QUANTITY`) AS TOTAL_QUANTITY\n",
        "    FROM {table_name}\n",
        "    GROUP BY `HOSPITAL TRUST CODE`, `BNF CODE`\n",
        "\"\"\"\n",
        "spark.sql(query).show(5)"
      ],
      "metadata": {
        "id": "8IyNiDBU8eiW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Joins\n",
        "We frequently need to join datasets together to enrich data or add dimensions to provide additional context for analysis."
      ],
      "metadata": {
        "id": "6BGJIJQ1UPUP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### PySpark\n",
        "In PySpark we can use the `join` method of a DataFrame, which takes arguments specifying:\n",
        "1. the other DataFrame to join\n",
        "2. the columns to join on (these can be a list of strings or Column objects)\n",
        "3. how to do the join (e.g. inner, left, etc)"
      ],
      "metadata": {
        "id": "lOHj4xaWUe37"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the mapping between codes and names\n",
        "# We'll join this to the aggregated data above to provide context\n",
        "bnf_hospital_cols = [\n",
        "    \"HOSPITAL_TRUST\",\n",
        "    \"HOSPITAL_TRUST_CODE\",\n",
        "    \"BNF_NAME\",\n",
        "    \"BNF_CODE\",\n",
        "]\n",
        "bnf_hospital_map_df = (\n",
        "    renamed_df.select(bnf_hospital_cols).distinct()\n",
        ")\n",
        "\n",
        "# It can be helpful in terms of readability/maintainability to pull the\n",
        "# join columns out of the join transformation\n",
        "join_on = [\n",
        "    \"HOSPITAL_TRUST_CODE\",\n",
        "    \"BNF_CODE\",\n",
        "]\n",
        "joined_df = (\n",
        "    totals_by_hospital_and_bnf_df\n",
        "    .join(\n",
        "        bnf_hospital_map_df,\n",
        "        on=join_on,\n",
        "        how=\"inner\"\n",
        "    )\n",
        ")\n",
        "\n",
        "joined_df.show(5)"
      ],
      "metadata": {
        "id": "QRnWEdWhUOzD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Spark SQL\n",
        "We can join in Spark SQL again using familiar syntax from other SQL flavours, e.g. 'INNER JOIN', 'LEFT JOIN' etc."
      ],
      "metadata": {
        "id": "2eiOxhs1XSRX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = f\"\"\"\n",
        "    SELECT\n",
        "        agg_tbl.HOSPITAL_TRUST_CODE,\n",
        "        agg_tbl.BNF_CODE,\n",
        "        agg_tbl.TOTAL_QUANTITY,\n",
        "        map_tbl.HOSPITAL_TRUST,\n",
        "        map_tbl.BNF_NAME\n",
        "    FROM (\n",
        "        SELECT\n",
        "            `HOSPITAL TRUST CODE` AS HOSPITAL_TRUST_CODE,\n",
        "            `BNF CODE` AS BNF_CODE,\n",
        "            sum(`TOTAL QUANTITY`) AS TOTAL_QUANTITY\n",
        "        FROM {table_name}\n",
        "        GROUP BY `HOSPITAL TRUST CODE`, `BNF CODE`\n",
        "    ) AS agg_tbl\n",
        "    INNER JOIN (\n",
        "        SELECT DISTINCT\n",
        "            `HOSPITAL TRUST CODE` AS HOSPITAL_TRUST_CODE,\n",
        "            `BNF CODE` AS BNF_CODE,\n",
        "            `HOSPITAL TRUST` AS HOSPITAL_TRUST,\n",
        "            `BNF NAME` AS BNF_NAME\n",
        "        FROM {table_name}\n",
        "    ) AS map_tbl\n",
        "    ON map_tbl.HOSPITAL_TRUST_CODE = agg_tbl.HOSPITAL_TRUST_CODE\n",
        "        AND map_tbl.BNF_CODE = agg_tbl.BNF_CODE\n",
        "\"\"\"\n",
        "spark.sql(query).show(5)"
      ],
      "metadata": {
        "id": "IS_bk4xQXhEn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Note on row ordering\n",
        "Notice that the order of rows is different between the PySpark and Spark SQL examples above.\n",
        "This demonstrates a key aspect to consider when writing code for Spark which is that the processing does not take account of row order unless you explictly tell it to (e.g. with an 'ORDER BY' clause).\n",
        "\n",
        "When Spark executes a task, it splits the rows up and sends them to different nodes in the cluster, each of which can take a different amount of time to process and return the results.\n",
        "\n",
        "Although the results are the same overall (as in they have the same schema and contain the same records), the order in which the records appear in the result sets is not guaranteed to be the same from run-to-run."
      ],
      "metadata": {
        "id": "5fXt_EvhdZ6z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Writing Outputs\n",
        "Spark is capable of writing outputs to a number of different formats, including csv, parquet and json.\n",
        "But most often we want to write to a managed table in a Spark SQL database (note the data will actually be stored in parquet files under the hood, but we will simply see a table in the database as we would in e.g. Oracle SQL Developer).\n"
      ],
      "metadata": {
        "id": "wXMzxR4rk7d7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### PySpark\n",
        "In PySpark we can use the `save` and `saveAsTable` methods of a DataFrame: the former writes records to files in a specified location (e.g. a network drive), while the latter creates a table in SQL to write the records.\n",
        "\n",
        "For general use `save` is more powerful since there are more options for configuration.\n",
        "But in the example below we'll keep it simple and use `saveAsTable`\n",
        "\n",
        "Note: since `saveAsTable` will error if a table exists, it can be useful to check if the table exists before you try to write to it.\n",
        "(Hint: this is probably good practice for other reasons too!). You can check if a table exists using `spark.catalog.tableExists`, passing the table name as the function arguments."
      ],
      "metadata": {
        "id": "tImAduII9QKM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# There are many options to configure the table to be written\n",
        "# We are only using two which provide the name and indicate\n",
        "# that we should overwrite the table if it already exists\n",
        "joined_df.write.saveAsTable(\"results_20250130\", mode=\"overwrite\")"
      ],
      "metadata": {
        "id": "csiO4iS-kteO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's take a look at our new table."
      ],
      "metadata": {
        "id": "J3MJrc39gDGY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.table(\"results_20250130\").show(5)  # Look at our new table!"
      ],
      "metadata": {
        "id": "gn-D8rWpfr2w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Spark SQL\n",
        "To do the same in pure Spark SQL requires much more code than above, because we must first create the table and then write into it.\n",
        "We will not cover this here, simply (and lazily) recommending that you use the line of PySpark shown above to write the table or follow the examples on the [Spark SQL docs](https://spark.apache.org/docs/latest/sql-ref-syntax-ddl-create-table-datasource.html)!"
      ],
      "metadata": {
        "id": "cWQ08nGM9RT3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example Pipeline\n",
        "Let's put our learning into practice and build a full end to end pipeline!\n",
        "\n",
        "Our pipeline will have the following steps:\n",
        "1. Read data\n",
        "1. Select columns of interest\n",
        "1. Derive a new column\n",
        "1. Filter the rows\n",
        "1. Aggregate the data\n",
        "1. Write outputs to a table"
      ],
      "metadata": {
        "id": "fg4uUm2gXQt0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuration\n",
        "input_table = \"monthly_hospital_data_oct24\"\n",
        "output_table = \"max_unit_cost_oct24_20250130\"\n",
        "\n",
        "# Input data\n",
        "input_df = spark.table(input_table)\n",
        "\n",
        "# Select columns of interest and rename\n",
        "selected_cols = [\n",
        "    F.col(\"HOSPITAL TRUST\").alias(\"HOSPITAL_TRUST\"),\n",
        "    F.col(\"BNF NAME\").alias(\"BNF_NAME\"),\n",
        "    F.col(\"TOTAL NIC\").alias(\"TOTAL_NIC\"),\n",
        "    F.col(\"TOTAL QUANTITY\").alias(\"TOTAL_QUANTITY\"),\n",
        "]\n",
        "selected_df = input_df.select(selected_cols)\n",
        "\n",
        "# Derive unit cost\n",
        "unit_cost_col = F.round(F.col(\"TOTAL_NIC\") / F.col(\"TOTAL_QUANTITY\"), 4)\n",
        "derived_df = selected_df.withColumn(\"UNIT_COST\", unit_cost_col)\n",
        "\n",
        "# Filter on BNF\n",
        "bnf_condition_col = F.col(\"BNF_NAME\").like(\"%Paracetamol 500mg tablets%\")\n",
        "filtered_df = derived_df.filter(bnf_condition_col)\n",
        "\n",
        "# Group by HOSPITAL, BNF and max UNIT_COST\n",
        "group_cols = [\n",
        "    \"HOSPITAL_TRUST\",\n",
        "    \"BNF_NAME\",\n",
        "]\n",
        "agg_cols = [\n",
        "    F.max(\"UNIT_COST\").alias(\"MAX_UNIT_COST\")\n",
        "]\n",
        "aggregated_df = filtered_df.groupBy(group_cols).agg(*agg_cols)\n",
        "\n",
        "# Write outputs\n",
        "aggregated_df.write.saveAsTable(output_table, mode=\"overwrite\")\n",
        "\n",
        "# Show outputs (ordered)\n",
        "(\n",
        "    spark\n",
        "    .table(output_table)\n",
        "    .orderBy(\"HOSPITAL_TRUST\", \"BNF_NAME\")\n",
        "    .show()\n",
        ")"
      ],
      "metadata": {
        "id": "tn7J0D8VXTAd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Takeaways\n",
        "\n",
        "### Key Takeaways\n",
        "- Spark is a technology for processing and transforming data at scale\n",
        "- Spark works by distributing work over a cluster of computers, or nodes.\n",
        "- You can interact with Spark through a number of APIs including PySpark (Python) and Spark SQL\n",
        "- We showed how to perform some of the fundamental operations in transforming data using PySpark and Spark SQL\n",
        "- And we built an example pipeline using what we've learned!\n",
        "\n",
        "## Additional Resources\n",
        "- [Apache Spark Documentation](https://spark.apache.org/docs/latest/)\n",
        "- [PySpark API Reference](https://spark.apache.org/docs/latest/api/python/index.html)\n",
        "- [Microsoft Fabric Training](https://learn.microsoft.com/en-us/training/modules/use-apache-spark-work-files-lakehouse/)\n",
        "- [Palantir Style Guide](https://github.com/palantir/pyspark-style-guide)\n"
      ],
      "metadata": {
        "id": "30QXr0w6km2Y"
      }
    }
  ]
}